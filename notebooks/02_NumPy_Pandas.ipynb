{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¢ NumPy & Pandas for Machine Learning\n",
    "\n",
    "This notebook covers essential NumPy and Pandas operations commonly used in machine learning interviews and data science workflows.\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Matrix Operations with NumPy](#matrix-operations)\n",
    "2. [PCA Implementation](#pca-implementation)\n",
    "3. [Cosine Similarity](#cosine-similarity)\n",
    "4. [Data Preprocessing with Pandas](#data-preprocessing)\n",
    "5. [Feature Engineering](#feature-engineering)\n",
    "6. [Practice Problems](#practice-problems)\n",
    "7. [Interview Tips](#interview-tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Problem 1: Matrix Operations for ML\n",
    "\n",
    "**Problem Statement**: Implement common matrix operations used in machine learning algorithms.\n",
    "\n",
    "**Requirements**:\n",
    "- Batch matrix multiplication with broadcasting\n",
    "- Efficient computation using vectorization\n",
    "- Memory-optimized operations for large datasets\n",
    "- Numerical stability for edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixOperations:\n",
    "    \"\"\"Efficient matrix operations for machine learning.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_matrix_multiply(A, B):\n",
    "        \"\"\"\n",
    "        Multiply batches of matrices using broadcasting.\n",
    "        A: (batch_size, m, k) or (m, k)\n",
    "        B: (batch_size, k, n) or (k, n)\n",
    "        Returns: (batch_size, m, n)\n",
    "        \"\"\"\n",
    "        return np.matmul(A, B)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x, axis=-1):\n",
    "        \"\"\"\n",
    "        Numerically stable softmax implementation.\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance_matrix(X, Y=None):\n",
    "        \"\"\"\n",
    "        Compute pairwise Euclidean distances efficiently.\n",
    "        X: (n_samples_1, n_features)\n",
    "        Y: (n_samples_2, n_features) or None\n",
    "        Returns: (n_samples_1, n_samples_2)\n",
    "        \"\"\"\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        \n",
    "        # Using the identity: ||x-y||¬≤ = ||x||¬≤ + ||y||¬≤ - 2*x¬∑y\n",
    "        X_norm_sq = np.sum(X**2, axis=1, keepdims=True)\n",
    "        Y_norm_sq = np.sum(Y**2, axis=1, keepdims=True)\n",
    "        \n",
    "        distances_sq = X_norm_sq + Y_norm_sq.T - 2 * np.dot(X, Y.T)\n",
    "        \n",
    "        # Ensure non-negative due to floating point errors\n",
    "        distances_sq = np.maximum(distances_sq, 0)\n",
    "        \n",
    "        return np.sqrt(distances_sq)\n",
    "\n",
    "# Test matrix operations\n",
    "print(\"üß™ Testing Matrix Operations:\")\n",
    "\n",
    "# Test batch matrix multiplication\n",
    "A = np.random.randn(5, 3, 4)\n",
    "B = np.random.randn(5, 4, 2)\n",
    "result = MatrixOperations.batch_matrix_multiply(A, B)\n",
    "print(f\"Batch matrix multiplication: {A.shape} √ó {B.shape} = {result.shape}\")\n",
    "\n",
    "# Test softmax\n",
    "logits = np.array([[2.0, 1.0, 0.1], [1.0, 3.0, 0.2]])\n",
    "probabilities = MatrixOperations.softmax(logits)\n",
    "print(f\"\\nSoftmax test:\")\n",
    "print(f\"Input: {logits}\")\n",
    "print(f\"Output: {probabilities}\")\n",
    "print(f\"Sum of probabilities: {np.sum(probabilities, axis=1)}\")\n",
    "\n",
    "# Test distance matrix\n",
    "X = np.random.randn(100, 5)\n",
    "distances = MatrixOperations.euclidean_distance_matrix(X[:10])  # 10x10 for display\n",
    "print(f\"\\nDistance matrix shape: {distances.shape}\")\n",
    "print(f\"Diagonal elements (should be 0): {np.diag(distances)[:5]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Matrix operations test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matrix operations performance\n",
    "import time\n",
    "\n",
    "def benchmark_matrix_operations():\n",
    "    \"\"\"Benchmark different matrix operation approaches.\"\"\"\n",
    "    sizes = [100, 200, 500, 1000]\n",
    "    vectorized_times = []\n",
    "    loop_times = []\n",
    "    \n",
    "    for n in sizes:\n",
    "        X = np.random.randn(n, 10)\n",
    "        \n",
    "        # Vectorized approach\n",
    "        start = time.time()\n",
    "        _ = MatrixOperations.euclidean_distance_matrix(X)\n",
    "        vectorized_time = time.time() - start\n",
    "        vectorized_times.append(vectorized_time)\n",
    "        \n",
    "        # Loop-based approach (slower)\n",
    "        start = time.time()\n",
    "        distances_loop = np.zeros((n, n))\n",
    "        for i in range(min(n, 50)):  # Limit for demonstration\n",
    "            for j in range(min(n, 50)):\n",
    "                distances_loop[i, j] = np.linalg.norm(X[i] - X[j])\n",
    "        loop_time = time.time() - start\n",
    "        loop_times.append(loop_time)\n",
    "    \n",
    "    return sizes, vectorized_times, loop_times\n",
    "\n",
    "# Run benchmark\n",
    "sizes, vec_times, loop_times = benchmark_matrix_operations()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogy(sizes, vec_times, 'o-', label='Vectorized', alpha=0.8)\n",
    "plt.semilogy(sizes, loop_times, 's-', label='Loop-based (limited)', alpha=0.8)\n",
    "plt.xlabel('Matrix Size')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Visualize softmax behavior\n",
    "plt.subplot(1, 2, 2)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Create a simple 2-class softmax\n",
    "    logits = np.column_stack([x, np.zeros_like(x)])\n",
    "    probs = MatrixOperations.softmax(logits / temp, axis=1)\n",
    "    plt.plot(x, probs[:, 0], label=f'Temperature: {temp}', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Input Value')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Softmax with Different Temperatures')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Problem 2: PCA Implementation from Scratch\n",
    "\n",
    "**Problem Statement**: Implement Principal Component Analysis using SVD for dimensionality reduction.\n",
    "\n",
    "**Requirements**:\n",
    "- Compute principal components using SVD\n",
    "- Handle data centering and scaling\n",
    "- Calculate explained variance ratio\n",
    "- Efficient implementation for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFromScratch:\n",
    "    \"\"\"Principal Component Analysis implementation using SVD.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components_ = None\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "        self.mean_ = None\n",
    "        self.singular_values_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA model to data.\n",
    "        X: (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.n_components is None:\n",
    "            self.n_components = min(n_samples, n_features)\n",
    "        \n",
    "        # Center the data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # Perform SVD\n",
    "        U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "        \n",
    "        # Select top n_components\n",
    "        self.components_ = Vt[:self.n_components]\n",
    "        self.singular_values_ = s[:self.n_components]\n",
    "        \n",
    "        # Calculate explained variance\n",
    "        self.explained_variance_ = (s[:self.n_components] ** 2) / (n_samples - 1)\n",
    "        total_variance = np.sum((s ** 2)) / (n_samples - 1)\n",
    "        self.explained_variance_ratio_ = self.explained_variance_ / total_variance\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to lower dimensional space.\n",
    "        X: (n_samples, n_features)\n",
    "        Returns: (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        if self.components_ is None:\n",
    "            raise ValueError(\"PCA model must be fitted first.\")\n",
    "        \n",
    "        X_centered = X - self.mean_\n",
    "        return X_centered @ self.components_.T\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit PCA model and transform data.\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"\n",
    "        Transform data back to original space.\n",
    "        X_transformed: (n_samples, n_components)\n",
    "        Returns: (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        return X_transformed @ self.components_ + self.mean_\n",
    "\n",
    "# Test PCA implementation\n",
    "print(\"üß™ Testing PCA Implementation:\")\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 1000, 20\n",
    "X_original = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Add some correlation to make PCA interesting\n",
    "correlation_matrix = np.random.randn(n_features, n_features)\n",
    "X = X_original @ correlation_matrix\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCAFromScratch(n_components=5)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Transformed shape: {X_transformed.shape}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {np.sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "# Test reconstruction\n",
    "X_reconstructed = pca.inverse_transform(X_transformed)\n",
    "reconstruction_error = np.mean((X - X_reconstructed) ** 2)\n",
    "print(f\"Reconstruction error (MSE): {reconstruction_error:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ PCA test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results\n",
    "# Use Iris dataset for better visualization\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Apply PCA to Iris dataset\n",
    "pca_iris = PCAFromScratch(n_components=2)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Original features (first two)\n",
    "plt.subplot(1, 3, 1)\n",
    "scatter = plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, alpha=0.7)\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title('Original Features')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: PCA transformed\n",
    "plt.subplot(1, 3, 2)\n",
    "scatter = plt.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_iris, alpha=0.7)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA Transformed')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Explained variance\n",
    "plt.subplot(1, 3, 3)\n",
    "components = range(1, len(pca_iris.explained_variance_ratio_) + 1)\n",
    "plt.bar(components, pca_iris.explained_variance_ratio_, alpha=0.7)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, v in enumerate(pca_iris.explained_variance_ratio_):\n",
    "    plt.text(i + 1, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä PCA Results for Iris Dataset:\")\n",
    "print(f\"Explained variance by PC1: {pca_iris.explained_variance_ratio_[0]:.3f}\")\n",
    "print(f\"Explained variance by PC2: {pca_iris.explained_variance_ratio_[1]:.3f}\")\n",
    "print(f\"Total explained variance: {np.sum(pca_iris.explained_variance_ratio_):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Problem 3: Cosine Similarity Matrix\n",
    "\n",
    "**Problem Statement**: Implement efficient cosine similarity computation for document/feature similarity.\n",
    "\n",
    "**Requirements**:\n",
    "- Compute pairwise cosine similarity\n",
    "- Handle zero vectors gracefully\n",
    "- Memory-efficient for large matrices\n",
    "- Vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityMetrics:\n",
    "    \"\"\"Collection of similarity metrics for ML applications.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity_matrix(X, Y=None):\n",
    "        \"\"\"\n",
    "        Compute pairwise cosine similarity matrix.\n",
    "        X: (n_samples_1, n_features)\n",
    "        Y: (n_samples_2, n_features) or None\n",
    "        Returns: (n_samples_1, n_samples_2)\n",
    "        \"\"\"\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        \n",
    "        # Normalize vectors to unit length\n",
    "        X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-10)\n",
    "        Y_norm = Y / (np.linalg.norm(Y, axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        # Compute cosine similarity using dot product\n",
    "        similarity = X_norm @ Y_norm.T\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    @staticmethod\n",
    "    def pearson_correlation_matrix(X, Y=None):\n",
    "        \"\"\"\n",
    "        Compute pairwise Pearson correlation matrix.\n",
    "        \"\"\"\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        \n",
    "        # Center the data\n",
    "        X_centered = X - np.mean(X, axis=1, keepdims=True)\n",
    "        Y_centered = Y - np.mean(Y, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute correlation using cosine similarity on centered data\n",
    "        return SimilarityMetrics.cosine_similarity_matrix(X_centered, Y_centered)\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard_similarity_matrix(X, Y=None):\n",
    "        \"\"\"\n",
    "        Compute Jaccard similarity for binary data.\n",
    "        X, Y should be binary matrices.\n",
    "        \"\"\"\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "        \n",
    "        # Convert to boolean for safety\n",
    "        X_bool = X.astype(bool)\n",
    "        Y_bool = Y.astype(bool)\n",
    "        \n",
    "        # Compute intersection and union\n",
    "        intersection = X_bool @ Y_bool.T\n",
    "        \n",
    "        X_sum = np.sum(X_bool, axis=1, keepdims=True)\n",
    "        Y_sum = np.sum(Y_bool, axis=1, keepdims=True)\n",
    "        union = X_sum + Y_sum.T - intersection\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        jaccard = intersection / (union + 1e-10)\n",
    "        \n",
    "        return jaccard\n",
    "\n",
    "# Test similarity metrics\n",
    "print(\"üß™ Testing Similarity Metrics:\")\n",
    "\n",
    "# Generate sample documents (as TF-IDF vectors)\n",
    "np.random.seed(42)\n",
    "n_docs, n_terms = 100, 50\n",
    "documents = np.random.exponential(0.5, (n_docs, n_terms))  # Sparse-like data\n",
    "documents[documents < 0.1] = 0  # Make it sparse\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = SimilarityMetrics.cosine_similarity_matrix(documents)\n",
    "print(f\"Cosine similarity matrix shape: {cosine_sim.shape}\")\n",
    "print(f\"Diagonal elements (self-similarity): {np.diag(cosine_sim)[:5]}\")\n",
    "print(f\"Average similarity: {np.mean(cosine_sim):.3f}\")\n",
    "\n",
    "# Test with binary data for Jaccard\n",
    "binary_data = (documents > 0.2).astype(int)\n",
    "jaccard_sim = SimilarityMetrics.jaccard_similarity_matrix(binary_data)\n",
    "print(f\"\\nJaccard similarity matrix shape: {jaccard_sim.shape}\")\n",
    "print(f\"Average Jaccard similarity: {np.mean(jaccard_sim):.3f}\")\n",
    "\n",
    "# Find most similar document pairs\n",
    "# Exclude diagonal (self-similarity)\n",
    "cosine_sim_no_diag = cosine_sim.copy()\n",
    "np.fill_diagonal(cosine_sim_no_diag, 0)\n",
    "\n",
    "# Find top 3 most similar pairs\n",
    "top_pairs = np.unravel_index(np.argpartition(cosine_sim_no_diag.ravel(), -3)[-3:], \n",
    "                             cosine_sim_no_diag.shape)\n",
    "print(f\"\\nTop 3 most similar document pairs:\")\n",
    "for i in range(3):\n",
    "    doc1, doc2 = top_pairs[0][i], top_pairs[1][i]\n",
    "    similarity = cosine_sim[doc1, doc2]\n",
    "    print(f\"  Documents {doc1} and {doc2}: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Similarity metrics test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrices and performance\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Cosine similarity heatmap (subset)\n",
    "plt.subplot(1, 3, 1)\n",
    "subset_size = 20\n",
    "sns.heatmap(cosine_sim[:subset_size, :subset_size], \n",
    "            cmap='coolwarm', center=0, \n",
    "            square=True, cbar_kws={'label': 'Cosine Similarity'})\n",
    "plt.title('Cosine Similarity Matrix (20x20 subset)')\n",
    "plt.xlabel('Document ID')\n",
    "plt.ylabel('Document ID')\n",
    "\n",
    "# Plot 2: Jaccard similarity heatmap (subset)\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.heatmap(jaccard_sim[:subset_size, :subset_size], \n",
    "            cmap='viridis', \n",
    "            square=True, cbar_kws={'label': 'Jaccard Similarity'})\n",
    "plt.title('Jaccard Similarity Matrix (20x20 subset)')\n",
    "plt.xlabel('Document ID')\n",
    "plt.ylabel('Document ID')\n",
    "\n",
    "# Plot 3: Similarity distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "# Get upper triangle values (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices_from(cosine_sim, k=1)\n",
    "cosine_values = cosine_sim[upper_tri_indices]\n",
    "jaccard_values = jaccard_sim[upper_tri_indices]\n",
    "\n",
    "plt.hist(cosine_values, bins=50, alpha=0.7, label='Cosine', density=True)\n",
    "plt.hist(jaccard_values, bins=50, alpha=0.7, label='Jaccard', density=True)\n",
    "plt.xlabel('Similarity Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Similarity Value Distributions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance comparison\n",
    "sizes = [50, 100, 200, 500]\n",
    "times = []\n",
    "\n",
    "for size in sizes:\n",
    "    data_subset = documents[:size]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    _ = SimilarityMetrics.cosine_similarity_matrix(data_subset)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    times.append(elapsed_time)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sizes, times, 'o-', alpha=0.8, linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Computation Time (seconds)')\n",
    "plt.title('Cosine Similarity Computation Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "for size, time_val in zip(sizes, times):\n",
    "    plt.annotate(f'{time_val:.3f}s', \n",
    "                xy=(size, time_val), \n",
    "                xytext=(5, 5), \n",
    "                textcoords='offset points',\n",
    "                fontsize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêº Problem 4: Data Preprocessing Pipeline\n",
    "\n",
    "**Problem Statement**: Create a comprehensive data preprocessing pipeline for real-world datasets.\n",
    "\n",
    "**Requirements**:\n",
    "- Handle missing values with different strategies\n",
    "- Encode categorical variables efficiently\n",
    "- Normalize and scale numerical features\n",
    "- Create date/time features\n",
    "- Pipeline should be reusable and configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"Comprehensive data preprocessing pipeline for ML workflows.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = None\n",
    "        self.feature_stats = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def handle_missing_values(self, df: pd.DataFrame, \n",
    "                            strategy: Dict[str, str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values with different strategies per column.\n",
    "        \n",
    "        strategy: dict mapping column names to strategies\n",
    "        Strategies: 'drop', 'mean', 'median', 'mode', 'forward_fill', 'backward_fill', 'constant'\n",
    "        \"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        for col, method in strategy.items():\n",
    "            if col not in df_processed.columns:\n",
    "                continue\n",
    "                \n",
    "            if method == 'drop':\n",
    "                df_processed = df_processed.dropna(subset=[col])\n",
    "            elif method == 'mean' and df_processed[col].dtype in ['int64', 'float64']:\n",
    "                fill_value = df_processed[col].mean()\n",
    "                df_processed[col].fillna(fill_value, inplace=True)\n",
    "                self.feature_stats[f'{col}_mean'] = fill_value\n",
    "            elif method == 'median' and df_processed[col].dtype in ['int64', 'float64']:\n",
    "                fill_value = df_processed[col].median()\n",
    "                df_processed[col].fillna(fill_value, inplace=True)\n",
    "                self.feature_stats[f'{col}_median'] = fill_value\n",
    "            elif method == 'mode':\n",
    "                fill_value = df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown'\n",
    "                df_processed[col].fillna(fill_value, inplace=True)\n",
    "                self.feature_stats[f'{col}_mode'] = fill_value\n",
    "            elif method == 'forward_fill':\n",
    "                df_processed[col].fillna(method='ffill', inplace=True)\n",
    "            elif method == 'backward_fill':\n",
    "                df_processed[col].fillna(method='bfill', inplace=True)\n",
    "            elif method == 'constant':\n",
    "                df_processed[col].fillna('Missing', inplace=True)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def encode_categorical_features(self, df: pd.DataFrame, \n",
    "                                  categorical_cols: List[str],\n",
    "                                  method: str = 'label') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encode categorical features.\n",
    "        Methods: 'label', 'onehot'\n",
    "        \"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col not in df_processed.columns:\n",
    "                continue\n",
    "            \n",
    "            if method == 'label':\n",
    "                if col not in self.label_encoders:\n",
    "                    self.label_encoders[col] = LabelEncoder()\n",
    "                    df_processed[col] = self.label_encoders[col].fit_transform(df_processed[col].astype(str))\n",
    "                else:\n",
    "                    # Handle unseen categories\n",
    "                    known_categories = set(self.label_encoders[col].classes_)\n",
    "                    df_processed[col] = df_processed[col].astype(str)\n",
    "                    \n",
    "                    # Replace unknown categories with most frequent known category\n",
    "                    unknown_mask = ~df_processed[col].isin(known_categories)\n",
    "                    if unknown_mask.any():\n",
    "                        most_frequent = df_processed[col][~unknown_mask].mode()\n",
    "                        if not most_frequent.empty:\n",
    "                            df_processed.loc[unknown_mask, col] = most_frequent.iloc[0]\n",
    "                    \n",
    "                    df_processed[col] = self.label_encoders[col].transform(df_processed[col])\n",
    "            \n",
    "            elif method == 'onehot':\n",
    "                # Create dummy variables\n",
    "                dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
    "                df_processed = pd.concat([df_processed.drop(col, axis=1), dummies], axis=1)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def scale_features(self, df: pd.DataFrame, \n",
    "                      numerical_cols: List[str],\n",
    "                      method: str = 'standard') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scale numerical features.\n",
    "        Methods: 'standard', 'minmax', 'robust'\n",
    "        \"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Filter columns that actually exist and are numerical\n",
    "        existing_numerical_cols = [col for col in numerical_cols \n",
    "                                 if col in df_processed.columns and \n",
    "                                 df_processed[col].dtype in ['int64', 'float64']]\n",
    "        \n",
    "        if not existing_numerical_cols:\n",
    "            return df_processed\n",
    "        \n",
    "        if method == 'standard':\n",
    "            if self.scaler is None:\n",
    "                self.scaler = StandardScaler()\n",
    "                df_processed[existing_numerical_cols] = self.scaler.fit_transform(\n",
    "                    df_processed[existing_numerical_cols])\n",
    "            else:\n",
    "                df_processed[existing_numerical_cols] = self.scaler.transform(\n",
    "                    df_processed[existing_numerical_cols])\n",
    "        \n",
    "        elif method == 'minmax':\n",
    "            if self.scaler is None:\n",
    "                self.scaler = MinMaxScaler()\n",
    "                df_processed[existing_numerical_cols] = self.scaler.fit_transform(\n",
    "                    df_processed[existing_numerical_cols])\n",
    "            else:\n",
    "                df_processed[existing_numerical_cols] = self.scaler.transform(\n",
    "                    df_processed[existing_numerical_cols])\n",
    "        \n",
    "        elif method == 'robust':\n",
    "            if self.scaler is None:\n",
    "                self.scaler = RobustScaler()\n",
    "                df_processed[existing_numerical_cols] = self.scaler.fit_transform(\n",
    "                    df_processed[existing_numerical_cols])\n",
    "            else:\n",
    "                df_processed[existing_numerical_cols] = self.scaler.transform(\n",
    "                    df_processed[existing_numerical_cols])\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def create_date_features(self, df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract useful features from datetime column.\n",
    "        \"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        if date_col in df_processed.columns:\n",
    "            # Convert to datetime\n",
    "            df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
    "            \n",
    "            # Extract features\n",
    "            df_processed[f'{date_col}_year'] = df_processed[date_col].dt.year\n",
    "            df_processed[f'{date_col}_month'] = df_processed[date_col].dt.month\n",
    "            df_processed[f'{date_col}_day'] = df_processed[date_col].dt.day\n",
    "            df_processed[f'{date_col}_dayofweek'] = df_processed[date_col].dt.dayofweek\n",
    "            df_processed[f'{date_col}_quarter'] = df_processed[date_col].dt.quarter\n",
    "            df_processed[f'{date_col}_is_weekend'] = (df_processed[date_col].dt.dayofweek >= 5).astype(int)\n",
    "            df_processed[f'{date_col}_is_month_start'] = df_processed[date_col].dt.is_month_start.astype(int)\n",
    "            df_processed[f'{date_col}_is_month_end'] = df_processed[date_col].dt.is_month_end.astype(int)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame, config: Dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Full preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        if 'missing_strategy' in config:\n",
    "            df_processed = self.handle_missing_values(df_processed, config['missing_strategy'])\n",
    "        \n",
    "        # Create date features\n",
    "        if 'date_columns' in config:\n",
    "            for date_col in config['date_columns']:\n",
    "                df_processed = self.create_date_features(df_processed, date_col)\n",
    "        \n",
    "        # Encode categorical features\n",
    "        if 'categorical_columns' in config:\n",
    "            df_processed = self.encode_categorical_features(\n",
    "                df_processed, \n",
    "                config['categorical_columns'],\n",
    "                config.get('categorical_method', 'label')\n",
    "            )\n",
    "        \n",
    "        # Scale numerical features\n",
    "        if 'numerical_columns' in config:\n",
    "            df_processed = self.scale_features(\n",
    "                df_processed,\n",
    "                config['numerical_columns'],\n",
    "                config.get('scaling_method', 'standard')\n",
    "            )\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return df_processed\n",
    "\n",
    "# Create sample dataset for testing\n",
    "print(\"üß™ Creating Sample Dataset for Preprocessing:\")\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create sample data with various data types and missing values\n",
    "sample_data = {\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'salary': np.random.lognormal(10, 1, n_samples),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples),\n",
    "    'date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
    "    'score': np.random.normal(75, 15, n_samples),\n",
    "    'binary_feature': np.random.choice([0, 1], n_samples)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "\n",
    "# Introduce missing values randomly\n",
    "missing_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "df_sample.loc[missing_indices[:50], 'age'] = np.nan\n",
    "df_sample.loc[missing_indices[50:], 'category'] = np.nan\n",
    "\n",
    "print(f\"Original dataset shape: {df_sample.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(df_sample.isnull().sum())\n",
    "print(f\"\\nData types:\")\n",
    "print(df_sample.dtypes)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing pipeline\n",
    "print(\"üîß Applying Preprocessing Pipeline:\")\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Define preprocessing configuration\n",
    "config = {\n",
    "    'missing_strategy': {\n",
    "        'age': 'median',\n",
    "        'category': 'mode',\n",
    "        'salary': 'mean'\n",
    "    },\n",
    "    'date_columns': ['date'],\n",
    "    'categorical_columns': ['category'],\n",
    "    'numerical_columns': ['age', 'salary', 'score'],\n",
    "    'categorical_method': 'label',\n",
    "    'scaling_method': 'standard'\n",
    "}\n",
    "\n",
    "# Apply preprocessing\n",
    "df_processed = preprocessor.fit_transform(df_sample, config)\n",
    "\n",
    "print(f\"Processed dataset shape: {df_processed.shape}\")\n",
    "print(f\"Missing values after processing:\")\n",
    "print(df_processed.isnull().sum())\n",
    "print(f\"\\nNew columns created:\")\n",
    "new_columns = set(df_processed.columns) - set(df_sample.columns)\n",
    "print(list(new_columns))\n",
    "print(f\"\\nProcessed data types:\")\n",
    "print(df_processed.dtypes)\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preprocessing results\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Before and after missing values\n",
    "plt.subplot(2, 3, 1)\n",
    "missing_before = df_sample.isnull().sum()\n",
    "missing_after = df_processed.isnull().sum()\n",
    "\n",
    "x = range(len(missing_before))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x], missing_before.values, width, label='Before', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in x], missing_after.values, width, label='After', alpha=0.7)\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Missing Values Count')\n",
    "plt.title('Missing Values Before and After')\n",
    "plt.xticks(x, missing_before.index, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Distribution of numerical features before scaling\n",
    "plt.subplot(2, 3, 2)\n",
    "numerical_cols = ['age', 'salary', 'score']\n",
    "df_sample[numerical_cols].boxplot(ax=plt.gca())\n",
    "plt.title('Numerical Features Before Scaling')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Original Values')\n",
    "\n",
    "# Plot 3: Distribution of numerical features after scaling\n",
    "plt.subplot(2, 3, 3)\n",
    "scaled_numerical_cols = [col for col in numerical_cols if col in df_processed.columns]\n",
    "if scaled_numerical_cols:\n",
    "    df_processed[scaled_numerical_cols].boxplot(ax=plt.gca())\n",
    "plt.title('Numerical Features After Scaling')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Scaled Values')\n",
    "\n",
    "# Plot 4: Categorical encoding results\n",
    "plt.subplot(2, 3, 4)\n",
    "if 'category' in df_processed.columns:\n",
    "    category_counts = df_processed['category'].value_counts()\n",
    "    plt.bar(category_counts.index, category_counts.values, alpha=0.7)\n",
    "    plt.xlabel('Encoded Category Values')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Categorical Feature After Encoding')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Date features created\n",
    "plt.subplot(2, 3, 5)\n",
    "date_features = [col for col in df_processed.columns if 'date_' in col and col != 'date']\n",
    "if date_features:\n",
    "    # Show distribution of one date feature\n",
    "    feature_to_plot = 'date_month' if 'date_month' in date_features else date_features[0]\n",
    "    df_processed[feature_to_plot].value_counts().sort_index().plot(kind='bar', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of {feature_to_plot}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Feature correlation heatmap\n",
    "plt.subplot(2, 3, 6)\n",
    "# Select numerical columns for correlation\n",
    "corr_cols = [col for col in df_processed.columns \n",
    "             if df_processed[col].dtype in ['int64', 'float64'] and not col.startswith('date_')]\n",
    "if len(corr_cols) > 1:\n",
    "    correlation_matrix = df_processed[corr_cols].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f', cbar_kws={'label': 'Correlation'})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"üìä Preprocessing Summary:\")\n",
    "print(f\"Original features: {len(df_sample.columns)}\")\n",
    "print(f\"Final features: {len(df_processed.columns)}\")\n",
    "print(f\"Features added: {len(df_processed.columns) - len(df_sample.columns)}\")\n",
    "print(f\"Missing values eliminated: {df_sample.isnull().sum().sum() - df_processed.isnull().sum().sum()}\")\n",
    "print(f\"Label encoders created: {len(preprocessor.label_encoders)}\")\n",
    "print(f\"Scaler fitted: {preprocessor.scaler is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Practice Problems\n",
    "\n",
    "Let's practice some additional NumPy and Pandas problems commonly seen in ML interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 5: Efficient K-Nearest Neighbors Distance Computation\n",
    "def knn_distances(X_train, X_test, k=5):\n",
    "    \"\"\"\n",
    "    Find k-nearest neighbors efficiently using vectorized operations.\n",
    "    \n",
    "    Time Complexity: O(n_test * n_train)\n",
    "    Space Complexity: O(n_test * n_train)\n",
    "    \"\"\"\n",
    "    # Compute distance matrix\n",
    "    distances = MatrixOperations.euclidean_distance_matrix(X_test, X_train)\n",
    "    \n",
    "    # Find k smallest distances and their indices\n",
    "    k_nearest_indices = np.argpartition(distances, k-1, axis=1)[:, :k]\n",
    "    k_nearest_distances = np.take_along_axis(distances, k_nearest_indices, axis=1)\n",
    "    \n",
    "    # Sort within k-nearest\n",
    "    sort_indices = np.argsort(k_nearest_distances, axis=1)\n",
    "    k_nearest_indices_sorted = np.take_along_axis(k_nearest_indices, sort_indices, axis=1)\n",
    "    k_nearest_distances_sorted = np.take_along_axis(k_nearest_distances, sort_indices, axis=1)\n",
    "    \n",
    "    return k_nearest_distances_sorted, k_nearest_indices_sorted\n",
    "\n",
    "# Test KNN implementation\n",
    "print(\"üß™ Testing KNN Distance Computation:\")\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(1000, 10)\n",
    "X_test = np.random.randn(100, 10)\n",
    "k = 5\n",
    "\n",
    "# Compute k-nearest neighbors\n",
    "start_time = time.time()\n",
    "knn_dists, knn_indices = knn_distances(X_train, X_test, k)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Computed {k}-NN for {len(X_test)} test points in {elapsed_time:.4f} seconds\")\n",
    "print(f\"KNN distances shape: {knn_dists.shape}\")\n",
    "print(f\"KNN indices shape: {knn_indices.shape}\")\n",
    "print(f\"Sample distances for first test point: {knn_dists[0]}\")\n",
    "print(f\"Sample indices for first test point: {knn_indices[0]}\")\n",
    "\n",
    "# Verify distances are sorted\n",
    "is_sorted = np.all(np.diff(knn_dists, axis=1) >= 0)\n",
    "print(f\"Distances are properly sorted: {is_sorted}\")\n",
    "\n",
    "print(\"\\n‚úÖ KNN distance computation test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 6: Outlier Detection using IQR Method\n",
    "def detect_outliers_iqr(data, columns=None, factor=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: pandas DataFrame\n",
    "    - columns: list of columns to check (None for all numerical columns)\n",
    "    - factor: IQR multiplier (typically 1.5)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with outlier flags and cleaned data\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        \n",
    "        outlier_info[col] = {\n",
    "            'count': outliers_mask.sum(),\n",
    "            'percentage': (outliers_mask.sum() / len(df)) * 100,\n",
    "            'bounds': (lower_bound, upper_bound),\n",
    "            'outlier_values': df.loc[outliers_mask, col].values\n",
    "        }\n",
    "        \n",
    "        # Add outlier flag column\n",
    "        df[f'{col}_is_outlier'] = outliers_mask\n",
    "    \n",
    "    return df, outlier_info\n",
    "\n",
    "# Test outlier detection\n",
    "print(\"üß™ Testing Outlier Detection:\")\n",
    "\n",
    "# Create sample data with outliers\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "normal_data = np.random.normal(50, 10, n_samples)\n",
    "# Inject some outliers\n",
    "outlier_indices = np.random.choice(n_samples, size=50, replace=False)\n",
    "normal_data[outlier_indices] = np.random.choice([0, 100], size=50)  # Extreme values\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'feature1': normal_data,\n",
    "    'feature2': np.random.exponential(2, n_samples),  # Naturally skewed\n",
    "    'feature3': np.random.normal(0, 1, n_samples)     # Standard normal\n",
    "})\n",
    "\n",
    "# Detect outliers\n",
    "df_with_outliers, outlier_stats = detect_outliers_iqr(test_df)\n",
    "\n",
    "print(\"Outlier Detection Results:\")\n",
    "for col, stats in outlier_stats.items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Outliers found: {stats['count']} ({stats['percentage']:.2f}%)\")\n",
    "    print(f\"  Normal range: [{stats['bounds'][0]:.2f}, {stats['bounds'][1]:.2f}]\")\n",
    "    if stats['count'] > 0:\n",
    "        print(f\"  Sample outlier values: {stats['outlier_values'][:5]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Outlier detection test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outlier detection results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "features = ['feature1', 'feature2', 'feature3']\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, (feature, color) in enumerate(zip(features, colors)):\n",
    "    # Box plot\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Normal data\n",
    "    normal_data = df_with_outliers[~df_with_outliers[f'{feature}_is_outlier']][feature]\n",
    "    outlier_data = df_with_outliers[df_with_outliers[f'{feature}_is_outlier']][feature]\n",
    "    \n",
    "    plt.boxplot(df_with_outliers[feature], patch_artist=True, \n",
    "                boxprops=dict(facecolor=color, alpha=0.7))\n",
    "    \n",
    "    # Add outlier points\n",
    "    if len(outlier_data) > 0:\n",
    "        plt.scatter([1] * len(outlier_data), outlier_data, \n",
    "                   color='red', alpha=0.6, s=20, label=f'Outliers ({len(outlier_data)})')\n",
    "    \n",
    "    plt.title(f'{feature} - Box Plot with Outliers')\n",
    "    plt.ylabel('Value')\n",
    "    if len(outlier_data) > 0:\n",
    "        plt.legend()\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(2, 3, i + 4)\n",
    "    \n",
    "    plt.hist(normal_data, bins=50, alpha=0.7, color=color, label='Normal', density=True)\n",
    "    if len(outlier_data) > 0:\n",
    "        plt.hist(outlier_data, bins=20, alpha=0.7, color='red', label='Outliers', density=True)\n",
    "    \n",
    "    # Add bounds\n",
    "    bounds = outlier_stats[feature]['bounds']\n",
    "    plt.axvline(bounds[0], color='orange', linestyle='--', alpha=0.8, label='IQR Bounds')\n",
    "    plt.axvline(bounds[1], color='orange', linestyle='--', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'{feature} - Distribution with IQR Bounds')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"üìä Outlier Detection Summary:\")\n",
    "total_outliers = sum(stats['count'] for stats in outlier_stats.values())\n",
    "total_data_points = len(df_with_outliers) * len(features)\n",
    "print(f\"Total outliers detected: {total_outliers}\")\n",
    "print(f\"Total data points examined: {total_data_points}\")\n",
    "print(f\"Overall outlier rate: {(total_outliers / total_data_points) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Interview Tips\n",
    "\n",
    "### üéØ NumPy Best Practices\n",
    "1. **Vectorization over loops** - Always prefer NumPy operations over Python loops\n",
    "2. **Broadcasting** - Understand how NumPy handles operations on arrays of different shapes\n",
    "3. **Memory efficiency** - Use views instead of copies when possible\n",
    "4. **Numerical stability** - Handle edge cases like division by zero\n",
    "5. **Data types** - Choose appropriate dtypes to save memory\n",
    "\n",
    "### üêº Pandas Best Practices\n",
    "1. **Vectorized operations** - Use pandas methods instead of apply() when possible\n",
    "2. **Memory optimization** - Use categorical data types for string columns with few unique values\n",
    "3. **Missing data handling** - Understand different strategies and their implications\n",
    "4. **Chaining operations** - Use method chaining for readable data transformations\n",
    "5. **Index usage** - Leverage indices for efficient data access\n",
    "\n",
    "### ‚ö° Performance Tips\n",
    "- Use `np.dot()` or `@` for matrix multiplication\n",
    "- Preallocate arrays when possible\n",
    "- Use `pd.cut()` and `pd.qcut()` for binning operations\n",
    "- Leverage `pd.crosstab()` and `pd.pivot_table()` for aggregations\n",
    "- Use `numba` or `cython` for performance-critical loops\n",
    "\n",
    "### üóÉÔ∏è Common Data Operations\n",
    "- **Reshaping**: `reshape()`, `transpose()`, `flatten()`\n",
    "- **Aggregation**: `sum()`, `mean()`, `std()`, `groupby()`\n",
    "- **Filtering**: Boolean indexing, `query()`, `where()`\n",
    "- **Joining**: `merge()`, `concat()`, `join()`\n",
    "- **Time series**: `resample()`, `rolling()`, `shift()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Different approaches to common operations\n",
    "import timeit\n",
    "\n",
    "def benchmark_operations():\n",
    "    \"\"\"Benchmark different approaches to common operations.\"\"\"\n",
    "    \n",
    "    # Setup data\n",
    "    n = 100000\n",
    "    arr = np.random.randn(n)\n",
    "    df = pd.DataFrame({'values': arr, 'groups': np.random.choice(['A', 'B', 'C'], n)})\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Sum of squares: vectorized vs loop\n",
    "    def vectorized_sum_squares():\n",
    "        return np.sum(arr ** 2)\n",
    "    \n",
    "    def loop_sum_squares():\n",
    "        total = 0\n",
    "        for x in arr[:1000]:  # Limited for demo\n",
    "            total += x ** 2\n",
    "        return total\n",
    "    \n",
    "    results['Sum of Squares'] = {\n",
    "        'Vectorized': timeit.timeit(vectorized_sum_squares, number=1000),\n",
    "        'Loop (1K only)': timeit.timeit(loop_sum_squares, number=100)\n",
    "    }\n",
    "    \n",
    "    # 2. GroupBy operations: pandas vs manual\n",
    "    def pandas_groupby():\n",
    "        return df.groupby('groups')['values'].mean()\n",
    "    \n",
    "    def manual_groupby():\n",
    "        groups = {}\n",
    "        for group, value in zip(df['groups'], df['values']):\n",
    "            if group not in groups:\n",
    "                groups[group] = []\n",
    "            groups[group].append(value)\n",
    "        return {k: np.mean(v) for k, v in groups.items()}\n",
    "    \n",
    "    results['GroupBy Mean'] = {\n",
    "        'Pandas': timeit.timeit(pandas_groupby, number=100),\n",
    "        'Manual': timeit.timeit(manual_groupby, number=10)  # Much slower\n",
    "    }\n",
    "    \n",
    "    # 3. Boolean indexing vs query\n",
    "    def boolean_indexing():\n",
    "        return df[df['values'] > 0]\n",
    "    \n",
    "    def query_method():\n",
    "        return df.query('values > 0')\n",
    "    \n",
    "    results['Filtering'] = {\n",
    "        'Boolean Indexing': timeit.timeit(boolean_indexing, number=100),\n",
    "        'Query Method': timeit.timeit(query_method, number=100)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"‚ö° Running Performance Benchmarks:\")\n",
    "benchmark_results = benchmark_operations()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, (operation, times) in enumerate(benchmark_results.items()):\n",
    "    methods = list(times.keys())\n",
    "    exec_times = list(times.values())\n",
    "    \n",
    "    bars = axes[i].bar(methods, exec_times, alpha=0.7)\n",
    "    axes[i].set_ylabel('Time (seconds)')\n",
    "    axes[i].set_title(f'{operation} Performance')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, time_val in zip(bars, exec_times):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,\n",
    "                    f'{time_val:.4f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Performance Results:\")\n",
    "for operation, times in benchmark_results.items():\n",
    "    print(f\"\\n{operation}:\")\n",
    "    for method, time_val in times.items():\n",
    "        print(f\"  {method}: {time_val:.4f} seconds\")\n",
    "    \n",
    "    # Calculate speedup\n",
    "    if len(times) == 2:\n",
    "        methods = list(times.keys())\n",
    "        speedup = max(times.values()) / min(times.values())\n",
    "        print(f\"  Speedup: {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "‚úÖ **Matrix Operations** - Efficient batch operations, softmax, distance matrices  \n",
    "‚úÖ **PCA Implementation** - Dimensionality reduction using SVD  \n",
    "‚úÖ **Similarity Metrics** - Cosine, Pearson, Jaccard similarities  \n",
    "‚úÖ **Data Preprocessing** - Complete pipeline for real-world data  \n",
    "‚úÖ **Outlier Detection** - IQR method for anomaly detection  \n",
    "‚úÖ **Performance Optimization** - Vectorization vs loops benchmarking  \n",
    "\n",
    "### üöÄ Next Steps\n",
    "1. Practice implementing these algorithms from memory\n",
    "2. Try variations with different datasets\n",
    "3. Move on to scikit-learn algorithm implementations\n",
    "4. Apply preprocessing pipelines to real datasets\n",
    "\n",
    "### üìö Additional Practice\n",
    "- Implement other dimensionality reduction techniques (t-SNE, UMAP)\n",
    "- Create custom pandas aggregation functions\n",
    "- Build efficient recommendation system components\n",
    "- Implement time series preprocessing functions\n",
    "\n",
    "### üîë Key Interview Points\n",
    "- **Vectorization** is crucial for performance\n",
    "- **Memory management** matters with large datasets\n",
    "- **Numerical stability** prevents edge case failures\n",
    "- **Pipeline thinking** enables reusable, maintainable code\n",
    "\n",
    "**Ready for the next challenge! üí™**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}