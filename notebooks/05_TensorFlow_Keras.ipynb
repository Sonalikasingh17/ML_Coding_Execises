{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ TensorFlow & Keras Deep Learning\n",
    "\n",
    "This notebook covers advanced TensorFlow and Keras implementations commonly asked in deep learning engineering interviews.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "1. [Custom Layers and Models](#custom-layers-models)\n",
    "2. [Advanced CNN Architectures](#cnn-architectures)\n",
    "3. [Transfer Learning and Fine-tuning](#transfer-learning)\n",
    "4. [Custom Training Loops](#custom-training)\n",
    "5. [Model Optimization and Deployment](#optimization-deployment)\n",
    "6. [Practice Problems](#practice-problems)\n",
    "7. [Interview Tips](#interview-tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras (with fallback for environments without GPU)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "    from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ TensorFlow not available. Using NumPy implementations.\")\n",
    "    TF_AVAILABLE = False\n",
    "\n",
    "# Standard libraries\n",
    "from sklearn.datasets import load_digits, fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    tf.random.set_seed(42)\n",
    "    # Configure GPU if available\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"ðŸš€ GPU available: {len(gpus)} GPU(s)\")\n",
    "    else:\n",
    "        print(\"ðŸ–¥ï¸ Using CPU\")\n",
    "\n",
    "print(\"ðŸ“Š All libraries imported successfully!\")\n",
    "print(\"ðŸ”¥ Ready for TensorFlow/Keras implementations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§± Problem 1: Custom Layers and Models\n",
    "\n",
    "**Problem Statement**: Implement custom Keras layers and models with advanced functionality.\n",
    "\n",
    "**Requirements**:\n",
    "- Custom layer with trainable parameters\n",
    "- Model subclassing with custom forward pass\n",
    "- Custom loss functions and metrics\n",
    "- Advanced regularization techniques\n",
    "- Model serialization and loading\n",
    "\n",
    "**Key Concepts**: Layer subclassing, model subclassing, custom training, regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Custom Dense Layer with additional features\n",
    "    class CustomDenseLayer(layers.Layer):\n",
    "        \"\"\"Custom dense layer with advanced features.\"\"\"\n",
    "        \n",
    "        def __init__(self, units, activation=None, use_bias=True, \n",
    "                     dropout_rate=0.0, l2_reg=0.0, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.units = units\n",
    "            self.activation = keras.activations.get(activation)\n",
    "            self.use_bias = use_bias\n",
    "            self.dropout_rate = dropout_rate\n",
    "            self.l2_reg = l2_reg\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            \"\"\"Create the layer's parameters.\"\"\"\n",
    "            # Weight matrix\n",
    "            self.kernel = self.add_weight(\n",
    "                name='kernel',\n",
    "                shape=(input_shape[-1], self.units),\n",
    "                initializer='glorot_uniform',\n",
    "                regularizer=keras.regularizers.l2(self.l2_reg) if self.l2_reg > 0 else None,\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            # Bias vector\n",
    "            if self.use_bias:\n",
    "                self.bias = self.add_weight(\n",
    "                    name='bias',\n",
    "                    shape=(self.units,),\n",
    "                    initializer='zeros',\n",
    "                    trainable=True\n",
    "                )\n",
    "            \n",
    "            # Dropout layer\n",
    "            if self.dropout_rate > 0:\n",
    "                self.dropout = layers.Dropout(self.dropout_rate)\n",
    "            \n",
    "            super().build(input_shape)\n",
    "        \n",
    "        def call(self, inputs, training=None):\n",
    "            \"\"\"Forward pass through the layer.\"\"\"\n",
    "            # Linear transformation\n",
    "            outputs = tf.matmul(inputs, self.kernel)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "            \n",
    "            # Apply activation\n",
    "            if self.activation is not None:\n",
    "                outputs = self.activation(outputs)\n",
    "            \n",
    "            # Apply dropout during training\n",
    "            if self.dropout_rate > 0 and training:\n",
    "                outputs = self.dropout(outputs, training=training)\n",
    "            \n",
    "            return outputs\n",
    "        \n",
    "        def get_config(self):\n",
    "            \"\"\"Return layer configuration for serialization.\"\"\"\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'units': self.units,\n",
    "                'activation': keras.activations.serialize(self.activation),\n",
    "                'use_bias': self.use_bias,\n",
    "                'dropout_rate': self.dropout_rate,\n",
    "                'l2_reg': self.l2_reg\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    # Custom Attention Layer\n",
    "    class MultiHeadAttention(layers.Layer):\n",
    "        \"\"\"Multi-head attention mechanism.\"\"\"\n",
    "        \n",
    "        def __init__(self, num_heads, key_dim, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.num_heads = num_heads\n",
    "            self.key_dim = key_dim\n",
    "            self.depth = key_dim // num_heads\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            self.wq = layers.Dense(self.key_dim)\n",
    "            self.wk = layers.Dense(self.key_dim)\n",
    "            self.wv = layers.Dense(self.key_dim)\n",
    "            self.dense = layers.Dense(input_shape[-1])\n",
    "            super().build(input_shape)\n",
    "        \n",
    "        def split_heads(self, x, batch_size):\n",
    "            x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "            return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            \n",
    "            q = self.split_heads(self.wq(inputs), batch_size)\n",
    "            k = self.split_heads(self.wk(inputs), batch_size)\n",
    "            v = self.split_heads(self.wv(inputs), batch_size)\n",
    "            \n",
    "            # Scaled dot-product attention\n",
    "            matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "            dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "            scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "            attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "            \n",
    "            output = tf.matmul(attention_weights, v)\n",
    "            output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "            concat_attention = tf.reshape(output, (batch_size, -1, self.key_dim))\n",
    "            \n",
    "            return self.dense(concat_attention)\n",
    "    \n",
    "    # Custom Model with advanced architecture\n",
    "    class AdvancedClassifier(keras.Model):\n",
    "        \"\"\"Custom model with advanced architecture.\"\"\"\n",
    "        \n",
    "        def __init__(self, num_classes, use_attention=False, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.num_classes = num_classes\n",
    "            self.use_attention = use_attention\n",
    "            \n",
    "            # Input processing\n",
    "            self.input_norm = layers.LayerNormalization()\n",
    "            \n",
    "            # Feature extraction layers\n",
    "            self.dense1 = CustomDenseLayer(128, activation='relu', dropout_rate=0.3, l2_reg=0.01)\n",
    "            self.dense2 = CustomDenseLayer(64, activation='relu', dropout_rate=0.2, l2_reg=0.01)\n",
    "            \n",
    "            # Optional attention mechanism\n",
    "            if use_attention:\n",
    "                self.reshape_for_attention = layers.Reshape((-1, 64))\n",
    "                self.attention = MultiHeadAttention(num_heads=4, key_dim=64)\n",
    "                self.global_pool = layers.GlobalAveragePooling1D()\n",
    "            \n",
    "            # Output layers\n",
    "            self.dropout = layers.Dropout(0.5)\n",
    "            self.classifier = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "        def call(self, inputs, training=None):\n",
    "            # Normalize inputs\n",
    "            x = self.input_norm(inputs)\n",
    "            \n",
    "            # Feature extraction\n",
    "            x = self.dense1(x, training=training)\n",
    "            x = self.dense2(x, training=training)\n",
    "            \n",
    "            # Optional attention mechanism\n",
    "            if self.use_attention:\n",
    "                x = self.reshape_for_attention(x)\n",
    "                x = self.attention(x)\n",
    "                x = self.global_pool(x)\n",
    "            \n",
    "            # Final classification\n",
    "            x = self.dropout(x, training=training)\n",
    "            return self.classifier(x)\n",
    "        \n",
    "        def model_summary(self):\n",
    "            \"\"\"Custom summary method.\"\"\"\n",
    "            x = keras.Input(shape=(self.input_spec.shape[1],))\n",
    "            model = keras.Model(inputs=[x], outputs=self.call(x))\n",
    "            return model.summary()\n",
    "    \n",
    "    # Custom Loss Functions\n",
    "    class FocalLoss(keras.losses.Loss):\n",
    "        \"\"\"Focal loss for handling class imbalance.\"\"\"\n",
    "        \n",
    "        def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "        \n",
    "        def call(self, y_true, y_pred):\n",
    "            # Compute cross entropy\n",
    "            ce_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "            \n",
    "            # Compute p_t\n",
    "            y_true = tf.cast(y_true, tf.int32)\n",
    "            p_t = tf.gather(y_pred, y_true, batch_dims=1)\n",
    "            \n",
    "            # Compute focal weight\n",
    "            alpha_t = self.alpha\n",
    "            focal_weight = alpha_t * tf.pow(1 - p_t, self.gamma)\n",
    "            \n",
    "            return focal_weight * ce_loss\n",
    "    \n",
    "    # Custom Metrics\n",
    "    class TopKAccuracy(keras.metrics.Metric):\n",
    "        \"\"\"Top-K accuracy metric.\"\"\"\n",
    "        \n",
    "        def __init__(self, k=3, name='top_k_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.k = k\n",
    "            self.total = self.add_weight(name='total', initializer='zeros')\n",
    "            self.count = self.add_weight(name='count', initializer='zeros')\n",
    "        \n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.int32)\n",
    "            top_k_pred = tf.nn.top_k(y_pred, k=self.k).indices\n",
    "            matches = tf.reduce_any(tf.equal(tf.expand_dims(y_true, -1), top_k_pred), axis=-1)\n",
    "            \n",
    "            self.total.assign_add(tf.reduce_sum(tf.cast(matches, tf.float32)))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "        \n",
    "        def result(self):\n",
    "            return self.total / self.count\n",
    "        \n",
    "        def reset_state(self):\n",
    "            self.total.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "    \n",
    "    print(\"ðŸ§± Custom layers and models defined successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ TensorFlow not available - skipping custom layer implementations\")\n",
    "    print(\"ðŸ’¡ In interview: Explain the concepts and show NumPy equivalents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Test Custom Layers and Models\n",
    "    print(\"ðŸ§ª Testing Custom Layers and Models:\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    # Use subset for faster training in demo\n",
    "    X_train = X_train[:5000].astype('float32') / 255.0\n",
    "    X_test = X_test[:1000].astype('float32') / 255.0\n",
    "    y_train = y_train[:5000].flatten()\n",
    "    y_test = y_test[:1000].flatten()\n",
    "    \n",
    "    # Flatten for fully connected network\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    print(f\"Training data shape: {X_train_flat.shape}\")\n",
    "    print(f\"Test data shape: {X_test_flat.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "    \n",
    "    # Create models with different configurations\n",
    "    models_config = [\n",
    "        {'name': 'Standard Model', 'use_attention': False},\n",
    "        {'name': 'Attention Model', 'use_attention': True}\n",
    "    ]\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for config in models_config:\n",
    "        print(f\"\\n=== Testing {config['name']} ===\")\n",
    "        \n",
    "        # Create model\n",
    "        model = AdvancedClassifier(\n",
    "            num_classes=10,\n",
    "            use_attention=config['use_attention']\n",
    "        )\n",
    "        \n",
    "        # Build model by calling it once\n",
    "        _ = model(X_train_flat[:1])\n",
    "        \n",
    "        # Compile with custom loss and metrics\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss=FocalLoss(alpha=0.25, gamma=2.0),\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                TopKAccuracy(k=3),\n",
    "                keras.metrics.SparseCategoricalAccuracy(name='sparse_acc')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"Model parameters: {model.count_params():,}\")\n",
    "        \n",
    "        # Define callbacks\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "        history = model.fit(\n",
    "            X_train_flat, y_train,\n",
    "            epochs=20,\n",
    "            batch_size=64,\n",
    "            validation_data=(X_test_flat, y_test),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_results = model.evaluate(X_test_flat, y_test, verbose=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_proba = model.predict(X_test_flat, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        \n",
    "        model_results.append({\n",
    "            'name': config['name'],\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'test_results': dict(zip(model.metrics_names, test_results)),\n",
    "            'training_time': training_time,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba,\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        })\n",
    "        \n",
    "        print(f\"Test Results: {dict(zip(model.metrics_names, test_results))}\")\n",
    "        print(f\"Training time: {training_time:.2f}s\")\n",
    "        print(f\"Epochs trained: {len(history.history['loss'])}\")\n",
    "    \n",
    "    # Test model serialization\n",
    "    print(\"\\nðŸ”„ Testing Model Serialization:\")\n",
    "    best_model = model_results[0]['model']\n",
    "    \n",
    "    # Save model\n",
    "    best_model.save_weights('custom_model_weights.h5')\n",
    "    print(\"âœ… Model weights saved\")\n",
    "    \n",
    "    # Create new model and load weights\n",
    "    new_model = AdvancedClassifier(num_classes=10, use_attention=False)\n",
    "    _ = new_model(X_test_flat[:1])  # Build model\n",
    "    new_model.load_weights('custom_model_weights.h5')\n",
    "    print(\"âœ… Model weights loaded\")\n",
    "    \n",
    "    # Verify predictions match\n",
    "    original_pred = best_model.predict(X_test_flat[:5], verbose=0)\n",
    "    loaded_pred = new_model.predict(X_test_flat[:5], verbose=0)\n",
    "    \n",
    "    if np.allclose(original_pred, loaded_pred, rtol=1e-6):\n",
    "        print(\"âœ… Model serialization successful - predictions match\")\n",
    "    else:\n",
    "        print(\"âŒ Model serialization failed - predictions don't match\")\n",
    "    \n",
    "    print(\"\\nâœ… Custom models testing completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping model testing - TensorFlow not available\")\n",
    "    model_results = []\n",
    "    print(\"ðŸ’¡ In interview: Discuss model architecture and training strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Problem 2: Advanced CNN Architectures\n",
    "\n",
    "**Problem Statement**: Implement modern CNN architectures with advanced techniques.\n",
    "\n",
    "**Requirements**:\n",
    "- ResNet-style skip connections\n",
    "- Depthwise separable convolutions\n",
    "- Squeeze-and-excitation blocks\n",
    "- Progressive resizing and mixed precision\n",
    "- Model ensemble techniques\n",
    "\n",
    "**Key Concepts**: Skip connections, efficient convolutions, attention mechanisms, model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Residual Block\n",
    "    class ResidualBlock(layers.Layer):\n",
    "        \"\"\"Residual block with skip connections.\"\"\"\n",
    "        \n",
    "        def __init__(self, filters, kernel_size=3, strides=1, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.filters = filters\n",
    "            self.kernel_size = kernel_size\n",
    "            self.strides = strides\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            # Main path\n",
    "            self.conv1 = layers.Conv2D(self.filters, self.kernel_size, \n",
    "                                     strides=self.strides, padding='same')\n",
    "            self.bn1 = layers.BatchNormalization()\n",
    "            self.conv2 = layers.Conv2D(self.filters, self.kernel_size, padding='same')\n",
    "            self.bn2 = layers.BatchNormalization()\n",
    "            \n",
    "            # Skip connection\n",
    "            if self.strides != 1 or input_shape[-1] != self.filters:\n",
    "                self.shortcut_conv = layers.Conv2D(self.filters, 1, strides=self.strides)\n",
    "                self.shortcut_bn = layers.BatchNormalization()\n",
    "            else:\n",
    "                self.shortcut_conv = None\n",
    "            \n",
    "            super().build(input_shape)\n",
    "        \n",
    "        def call(self, inputs, training=None):\n",
    "            # Main path\n",
    "            x = self.conv1(inputs)\n",
    "            x = self.bn1(x, training=training)\n",
    "            x = tf.nn.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x, training=training)\n",
    "            \n",
    "            # Skip connection\n",
    "            if self.shortcut_conv is not None:\n",
    "                shortcut = self.shortcut_conv(inputs)\n",
    "                shortcut = self.shortcut_bn(shortcut, training=training)\n",
    "            else:\n",
    "                shortcut = inputs\n",
    "            \n",
    "            # Add skip connection and apply ReLU\n",
    "            output = tf.nn.relu(x + shortcut)\n",
    "            return output\n",
    "    \n",
    "    # Squeeze-and-Excitation Block\n",
    "    class SEBlock(layers.Layer):\n",
    "        \"\"\"Squeeze-and-Excitation block for channel attention.\"\"\"\n",
    "        \n",
    "        def __init__(self, ratio=16, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.ratio = ratio\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            channels = input_shape[-1]\n",
    "            self.global_pool = layers.GlobalAveragePooling2D()\n",
    "            self.dense1 = layers.Dense(channels // self.ratio, activation='relu')\n",
    "            self.dense2 = layers.Dense(channels, activation='sigmoid')\n",
    "            self.reshape = layers.Reshape((1, 1, channels))\n",
    "            super().build(input_shape)\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            # Squeeze\n",
    "            se = self.global_pool(inputs)\n",
    "            \n",
    "            # Excitation\n",
    "            se = self.dense1(se)\n",
    "            se = self.dense2(se)\n",
    "            se = self.reshape(se)\n",
    "            \n",
    "            # Scale\n",
    "            return inputs * se\n",
    "    \n",
    "    # Depthwise Separable Convolution Block\n",
    "    class DepthwiseConvBlock(layers.Layer):\n",
    "        \"\"\"Depthwise separable convolution block.\"\"\"\n",
    "        \n",
    "        def __init__(self, filters, kernel_size=3, strides=1, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.filters = filters\n",
    "            self.kernel_size = kernel_size\n",
    "            self.strides = strides\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            # Depthwise convolution\n",
    "            self.depthwise = layers.DepthwiseConv2D(\n",
    "                self.kernel_size, strides=self.strides, padding='same'\n",
    "            )\n",
    "            self.bn1 = layers.BatchNormalization()\n",
    "            \n",
    "            # Pointwise convolution\n",
    "            self.pointwise = layers.Conv2D(self.filters, 1)\n",
    "            self.bn2 = layers.BatchNormalization()\n",
    "            \n",
    "            super().build(input_shape)\n",
    "        \n",
    "        def call(self, inputs, training=None):\n",
    "            # Depthwise convolution\n",
    "            x = self.depthwise(inputs)\n",
    "            x = self.bn1(x, training=training)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # Pointwise convolution\n",
    "            x = self.pointwise(x)\n",
    "            x = self.bn2(x, training=training)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    # Advanced CNN Model\n",
    "    class AdvancedCNN(keras.Model):\n",
    "        \"\"\"Advanced CNN with multiple architecture components.\"\"\"\n",
    "        \n",
    "        def __init__(self, num_classes, architecture='resnet', **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.num_classes = num_classes\n",
    "            self.architecture = architecture\n",
    "            \n",
    "            # Input processing\n",
    "            self.input_conv = layers.Conv2D(32, 3, padding='same')\n",
    "            self.input_bn = layers.BatchNormalization()\n",
    "            \n",
    "            # Architecture-specific layers\n",
    "            if architecture == 'resnet':\n",
    "                self.block1 = ResidualBlock(64, strides=2)\n",
    "                self.block2 = ResidualBlock(128, strides=2)\n",
    "                self.block3 = ResidualBlock(256, strides=2)\n",
    "                \n",
    "            elif architecture == 'mobilenet':\n",
    "                self.block1 = DepthwiseConvBlock(64, strides=2)\n",
    "                self.block2 = DepthwiseConvBlock(128, strides=2)\n",
    "                self.block3 = DepthwiseConvBlock(256, strides=2)\n",
    "                \n",
    "            elif architecture == 'se_resnet':\n",
    "                self.block1 = ResidualBlock(64, strides=2)\n",
    "                self.se1 = SEBlock()\n",
    "                self.block2 = ResidualBlock(128, strides=2)\n",
    "                self.se2 = SEBlock()\n",
    "                self.block3 = ResidualBlock(256, strides=2)\n",
    "                self.se3 = SEBlock()\n",
    "            \n",
    "            # Output layers\n",
    "            self.global_pool = layers.GlobalAveragePooling2D()\n",
    "            self.dropout = layers.Dropout(0.5)\n",
    "            self.classifier = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "        def call(self, inputs, training=None):\n",
    "            # Input processing\n",
    "            x = self.input_conv(inputs)\n",
    "            x = self.input_bn(x, training=training)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            # Architecture-specific forward pass\n",
    "            if self.architecture == 'resnet':\n",
    "                x = self.block1(x, training=training)\n",
    "                x = self.block2(x, training=training)\n",
    "                x = self.block3(x, training=training)\n",
    "                \n",
    "            elif self.architecture == 'mobilenet':\n",
    "                x = self.block1(x, training=training)\n",
    "                x = self.block2(x, training=training)\n",
    "                x = self.block3(x, training=training)\n",
    "                \n",
    "            elif self.architecture == 'se_resnet':\n",
    "                x = self.block1(x, training=training)\n",
    "                x = self.se1(x)\n",
    "                x = self.block2(x, training=training)\n",
    "                x = self.se2(x)\n",
    "                x = self.block3(x, training=training)\n",
    "                x = self.se3(x)\n",
    "            \n",
    "            # Output\n",
    "            x = self.global_pool(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "            return self.classifier(x)\n",
    "    \n",
    "    # Model Ensemble\n",
    "    class ModelEnsemble(keras.Model):\n",
    "        \"\"\"Ensemble of multiple models.\"\"\"\n",
    "        \n",
    "        def __init__(self, models, weights=None, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.models = models\n",
    "            self.weights = weights or [1.0 / len(models)] * len(models)\n",
    "        \n",
    "        def call(self, inputs, training=None):\n",
    "            predictions = []\n",
    "            for model in self.models:\n",
    "                pred = model(inputs, training=training)\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            # Weighted average of predictions\n",
    "            weighted_preds = [w * pred for w, pred in zip(self.weights, predictions)]\n",
    "            return tf.reduce_sum(weighted_preds, axis=0)\n",
    "    \n",
    "    print(\"ðŸ—ï¸ Advanced CNN architectures defined successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ TensorFlow not available - showing architecture concepts\")\n",
    "    print(\"ðŸ’¡ Key concepts: ResNet skip connections, SE blocks, depthwise convolutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Test Advanced CNN Architectures\n",
    "    print(\"ðŸ§ª Testing Advanced CNN Architectures:\")\n",
    "    \n",
    "    # Use smaller subset for faster training\n",
    "    X_train_cnn = X_train[:1000]\n",
    "    y_train_cnn = y_train[:1000]\n",
    "    X_test_cnn = X_test[:200]\n",
    "    y_test_cnn = y_test[:200]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_cnn)}\")\n",
    "    print(f\"Test samples: {len(X_test_cnn)}\")\n",
    "    print(f\"Image shape: {X_train_cnn.shape[1:]}\")\n",
    "    \n",
    "    # Test different architectures\n",
    "    architectures = ['resnet', 'mobilenet', 'se_resnet']\n",
    "    cnn_results = []\n",
    "    \n",
    "    for arch in architectures:\n",
    "        print(f\"\\n=== Testing {arch.upper()} Architecture ===\")\n",
    "        \n",
    "        # Create model\n",
    "        model_cnn = AdvancedCNN(num_classes=10, architecture=arch)\n",
    "        \n",
    "        # Build model\n",
    "        _ = model_cnn(X_train_cnn[:1])\n",
    "        \n",
    "        # Compile model\n",
    "        model_cnn.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"Model parameters: {model_cnn.count_params():,}\")\n",
    "        \n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "        history_cnn = model_cnn.fit(\n",
    "            X_train_cnn, y_train_cnn,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_test_cnn, y_test_cnn),\n",
    "            verbose=1\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_acc = model_cnn.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred_cnn = model_cnn.predict(X_test_cnn, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred_cnn, axis=1)\n",
    "        \n",
    "        cnn_results.append({\n",
    "            'architecture': arch,\n",
    "            'model': model_cnn,\n",
    "            'history': history_cnn,\n",
    "            'test_accuracy': test_acc,\n",
    "            'test_loss': test_loss,\n",
    "            'training_time': training_time,\n",
    "            'parameters': model_cnn.count_params(),\n",
    "            'predictions': y_pred_classes\n",
    "        })\n",
    "        \n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Training Time: {training_time:.2f}s\")\n",
    "    \n",
    "    # Create and test ensemble\n",
    "    print(\"\\n=== Testing Model Ensemble ===\")\n",
    "    \n",
    "    # Use top 2 models for ensemble\n",
    "    sorted_results = sorted(cnn_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "    ensemble_models = [result['model'] for result in sorted_results[:2]]\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble = ModelEnsemble(ensemble_models, weights=[0.6, 0.4])\n",
    "    \n",
    "    # Test ensemble\n",
    "    ensemble_pred = ensemble.predict(X_test_cnn, verbose=0)\n",
    "    ensemble_classes = np.argmax(ensemble_pred, axis=1)\n",
    "    ensemble_accuracy = accuracy_score(y_test_cnn, ensemble_classes)\n",
    "    \n",
    "    print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "    print(f\"Best Single Model: {sorted_results[0]['test_accuracy']:.4f}\")\n",
    "    print(f\"Ensemble Improvement: {ensemble_accuracy - sorted_results[0]['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Test mixed precision training (if supported)\n",
    "    try:\n",
    "        print(\"\\n=== Testing Mixed Precision Training ===\")\n",
    "        \n",
    "        # Enable mixed precision\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        \n",
    "        # Create model for mixed precision\n",
    "        model_mp = AdvancedCNN(num_classes=10, architecture='resnet')\n",
    "        _ = model_mp(X_train_cnn[:1])\n",
    "        \n",
    "        # Compile with loss scaling\n",
    "        model_mp.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train for a few epochs\n",
    "        start_time = time.time()\n",
    "        history_mp = model_mp.fit(\n",
    "            X_train_cnn[:500], y_train_cnn[:500],\n",
    "            epochs=3,\n",
    "            batch_size=32,\n",
    "            verbose=1\n",
    "        )\n",
    "        mp_training_time = time.time() - start_time\n",
    "        \n",
    "        # Reset to default policy\n",
    "        tf.keras.mixed_precision.set_global_policy('float32')\n",
    "        \n",
    "        print(f\"Mixed precision training completed in {mp_training_time:.2f}s\")\n",
    "        print(\"âœ… Mixed precision training successful\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Mixed precision not supported: {e}\")\n",
    "        tf.keras.mixed_precision.set_global_policy('float32')\n",
    "    \n",
    "    print(\"\\nâœ… Advanced CNN architecture testing completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping CNN testing - TensorFlow not available\")\n",
    "    cnn_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Problem 3: Transfer Learning and Fine-tuning\n",
    "\n",
    "**Problem Statement**: Implement transfer learning strategies with pre-trained models.\n",
    "\n",
    "**Requirements**:\n",
    "- Load and modify pre-trained models\n",
    "- Feature extraction vs fine-tuning\n",
    "- Layer freezing strategies\n",
    "- Progressive unfreezing\n",
    "- Domain adaptation techniques\n",
    "\n",
    "**Key Concepts**: Pre-trained models, feature extraction, gradual unfreezing, learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Transfer Learning Helper Class\n",
    "    class TransferLearningModel:\n",
    "        \"\"\"Helper class for transfer learning experiments.\"\"\"\n",
    "        \n",
    "        def __init__(self, base_model_name='resnet50', num_classes=10, input_shape=(32, 32, 3)):\n",
    "            self.base_model_name = base_model_name\n",
    "            self.num_classes = num_classes\n",
    "            self.input_shape = input_shape\n",
    "            self.model = None\n",
    "            self.base_model = None\n",
    "        \n",
    "        def create_model(self, strategy='feature_extraction', trainable_layers=0):\n",
    "            \"\"\"Create transfer learning model.\"\"\"\n",
    "            # Load pre-trained base model\n",
    "            if self.base_model_name == 'resnet50':\n",
    "                self.base_model = ResNet50(\n",
    "                    weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=self.input_shape\n",
    "                )\n",
    "            elif self.base_model_name == 'vgg16':\n",
    "                self.base_model = VGG16(\n",
    "                    weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=self.input_shape\n",
    "                )\n",
    "            elif self.base_model_name == 'mobilenet':\n",
    "                self.base_model = MobileNetV2(\n",
    "                    weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=self.input_shape\n",
    "                )\n",
    "            \n",
    "            # Set trainable layers based on strategy\n",
    "            if strategy == 'feature_extraction':\n",
    "                self.base_model.trainable = False\n",
    "            elif strategy == 'fine_tuning':\n",
    "                self.base_model.trainable = True\n",
    "            elif strategy == 'progressive':\n",
    "                # Freeze all layers initially\n",
    "                self.base_model.trainable = True\n",
    "                for layer in self.base_model.layers[:-trainable_layers]:\n",
    "                    layer.trainable = False\n",
    "            \n",
    "            # Add custom head\n",
    "            inputs = keras.Input(shape=self.input_shape)\n",
    "            \n",
    "            # Data augmentation (only during training)\n",
    "            x = layers.RandomFlip('horizontal')(inputs)\n",
    "            x = layers.RandomRotation(0.1)(x)\n",
    "            \n",
    "            # Base model\n",
    "            x = self.base_model(x, training=False if strategy == 'feature_extraction' else None)\n",
    "            \n",
    "            # Custom head\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "            x = layers.Dropout(0.3)(x)\n",
    "            x = layers.Dense(128, activation='relu')(x)\n",
    "            x = layers.Dropout(0.5)(x)\n",
    "            outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "            \n",
    "            self.model = keras.Model(inputs, outputs)\n",
    "            return self.model\n",
    "        \n",
    "        def progressive_unfreeze(self, step):\n",
    "            \"\"\"Progressively unfreeze layers.\"\"\"\n",
    "            if self.base_model is None:\n",
    "                return\n",
    "            \n",
    "            total_layers = len(self.base_model.layers)\n",
    "            layers_to_unfreeze = min(step * 5, total_layers)  # Unfreeze 5 layers at a time\n",
    "            \n",
    "            for i, layer in enumerate(self.base_model.layers):\n",
    "                if i >= total_layers - layers_to_unfreeze:\n",
    "                    layer.trainable = True\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "            \n",
    "            print(f\"Unfrozen last {layers_to_unfreeze} layers out of {total_layers}\")\n",
    "        \n",
    "        def get_trainable_params(self):\n",
    "            \"\"\"Get number of trainable parameters.\"\"\"\n",
    "            if self.model is None:\n",
    "                return 0\n",
    "            return sum([tf.size(w).numpy() for w in self.model.trainable_weights])\n",
    "    \n",
    "    # Custom Learning Rate Scheduler\n",
    "    class WarmupCosineDecay(keras.optimizers.schedules.LearningRateSchedule):\n",
    "        \"\"\"Warmup followed by cosine decay schedule.\"\"\"\n",
    "        \n",
    "        def __init__(self, initial_learning_rate, decay_steps, warmup_steps=1000, alpha=0.0):\n",
    "            super().__init__()\n",
    "            self.initial_learning_rate = initial_learning_rate\n",
    "            self.decay_steps = decay_steps\n",
    "            self.warmup_steps = warmup_steps\n",
    "            self.alpha = alpha\n",
    "        \n",
    "        def __call__(self, step):\n",
    "            if step < self.warmup_steps:\n",
    "                # Linear warmup\n",
    "                return self.initial_learning_rate * step / self.warmup_steps\n",
    "            else:\n",
    "                # Cosine decay\n",
    "                step = tf.cast(step - self.warmup_steps, tf.float32)\n",
    "                decay_steps = tf.cast(self.decay_steps - self.warmup_steps, tf.float32)\n",
    "                cosine_decay = 0.5 * (1 + tf.cos(np.pi * step / decay_steps))\n",
    "                return self.initial_learning_rate * cosine_decay + self.alpha\n",
    "    \n",
    "    print(\"ðŸ”„ Transfer learning components defined successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ TensorFlow not available - showing transfer learning concepts\")\n",
    "    print(\"ðŸ’¡ Key concepts: Feature extraction, fine-tuning, progressive unfreezing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Test Transfer Learning\n",
    "    print(\"ðŸ§ª Testing Transfer Learning Strategies:\")\n",
    "    \n",
    "    # Use even smaller subset for transfer learning demo\n",
    "    X_train_tl = X_train[:500]\n",
    "    y_train_tl = y_train[:500]\n",
    "    X_test_tl = X_test[:100]\n",
    "    y_test_tl = y_test[:100]\n",
    "    \n",
    "    print(f\"Transfer learning - Training samples: {len(X_train_tl)}\")\n",
    "    print(f\"Transfer learning - Test samples: {len(X_test_tl)}\")\n",
    "    \n",
    "    # Test different transfer learning strategies\n",
    "    tl_strategies = [\n",
    "        {'name': 'Feature Extraction', 'strategy': 'feature_extraction'},\n",
    "        {'name': 'Fine Tuning', 'strategy': 'fine_tuning'},\n",
    "        {'name': 'Progressive', 'strategy': 'progressive', 'trainable_layers': 10}\n",
    "    ]\n",
    "    \n",
    "    tl_results = []\n",
    "    \n",
    "    for config in tl_strategies:\n",
    "        try:\n",
    "            print(f\"\\n=== Testing {config['name']} Strategy ===\")\n",
    "            \n",
    "            # Create transfer learning model\n",
    "            tl_model = TransferLearningModel(\n",
    "                base_model_name='mobilenet',  # Use MobileNet for faster training\n",
    "                num_classes=10,\n",
    "                input_shape=(32, 32, 3)\n",
    "            )\n",
    "            \n",
    "            model = tl_model.create_model(\n",
    "                strategy=config['strategy'],\n",
    "                trainable_layers=config.get('trainable_layers', 0)\n",
    "            )\n",
    "            \n",
    "            trainable_params = tl_model.get_trainable_params()\n",
    "            total_params = model.count_params()\n",
    "            \n",
    "            print(f\"Total parameters: {total_params:,}\")\n",
    "            print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "            print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "            \n",
    "            # Set learning rate based on strategy\n",
    "            if config['strategy'] == 'feature_extraction':\n",
    "                learning_rate = 0.001\n",
    "            else:\n",
    "                learning_rate = 0.0001  # Lower LR for fine-tuning\n",
    "            \n",
    "            # Use custom learning rate schedule\n",
    "            lr_schedule = WarmupCosineDecay(\n",
    "                initial_learning_rate=learning_rate,\n",
    "                decay_steps=1000,\n",
    "                warmup_steps=100\n",
    "            )\n",
    "            \n",
    "            # Compile model\n",
    "            model.compile(\n",
    "                optimizer=optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if config['strategy'] == 'progressive':\n",
    "                # Progressive unfreezing training\n",
    "                histories = []\n",
    "                for step in range(1, 3):  # 2 steps for demo\n",
    "                    print(f\"\\nProgressive step {step}:\")\n",
    "                    tl_model.progressive_unfreeze(step)\n",
    "                    \n",
    "                    # Recompile with potentially different learning rate\n",
    "                    current_lr = learning_rate / (2 ** (step - 1))  # Reduce LR each step\n",
    "                    model.compile(\n",
    "                        optimizer=optimizers.Adam(learning_rate=current_lr),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy']\n",
    "                    )\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        X_train_tl, y_train_tl,\n",
    "                        epochs=3,\n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test_tl, y_test_tl),\n",
    "                        verbose=1\n",
    "                    )\n",
    "                    histories.append(history)\n",
    "                \n",
    "                # Combine histories\n",
    "                combined_history = {\n",
    "                    'loss': [],\n",
    "                    'accuracy': [],\n",
    "                    'val_loss': [],\n",
    "                    'val_accuracy': []\n",
    "                }\n",
    "                for h in histories:\n",
    "                    for key in combined_history.keys():\n",
    "                        combined_history[key].extend(h.history[key])\n",
    "                \n",
    "                # Create mock history object\n",
    "                class MockHistory:\n",
    "                    def __init__(self, history_dict):\n",
    "                        self.history = history_dict\n",
    "                \n",
    "                history_tl = MockHistory(combined_history)\n",
    "                \n",
    "            else:\n",
    "                # Standard training\n",
    "                history_tl = model.fit(\n",
    "                    X_train_tl, y_train_tl,\n",
    "                    epochs=5,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_test_tl, y_test_tl),\n",
    "                    verbose=1\n",
    "                )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate model\n",
    "            test_loss, test_acc = model.evaluate(X_test_tl, y_test_tl, verbose=0)\n",
    "            \n",
    "            tl_results.append({\n",
    "                'name': config['name'],\n",
    "                'strategy': config['strategy'],\n",
    "                'model': model,\n",
    "                'history': history_tl,\n",
    "                'test_accuracy': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "                'training_time': training_time,\n",
    "                'trainable_params': trainable_params,\n",
    "                'total_params': total_params\n",
    "            })\n",
    "            \n",
    "            print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "            print(f\"Training Time: {training_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with {config['name']}: {str(e)}\")\n",
    "            print(\"ðŸ’¡ In real scenario, ensure proper model loading and GPU memory\")\n",
    "    \n",
    "    # Compare with training from scratch\n",
    "    try:\n",
    "        print(\"\\n=== Training from Scratch (Baseline) ===\")\n",
    "        \n",
    "        # Simple CNN from scratch\n",
    "        scratch_model = keras.Sequential([\n",
    "            layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3, activation='relu'),\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        scratch_model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"Scratch model parameters: {scratch_model.count_params():,}\")\n",
    "        \n",
    "        # Train scratch model\n",
    "        start_time = time.time()\n",
    "        scratch_history = scratch_model.fit(\n",
    "            X_train_tl, y_train_tl,\n",
    "            epochs=10,\n",
    "            batch_size=16,\n",
    "            validation_data=(X_test_tl, y_test_tl),\n",
    "            verbose=1\n",
    "        )\n",
    "        scratch_time = time.time() - start_time\n",
    "        \n",
    "        scratch_loss, scratch_acc = scratch_model.evaluate(X_test_tl, y_test_tl, verbose=0)\n",
    "        \n",
    "        print(f\"Scratch model accuracy: {scratch_acc:.4f}\")\n",
    "        print(f\"Training time: {scratch_time:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with scratch model: {str(e)}\")\n",
    "        scratch_acc = 0.0\n",
    "    \n",
    "    print(\"\\nâœ… Transfer learning testing completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping transfer learning tests - TensorFlow not available\")\n",
    "    tl_results = []\n",
    "    scratch_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Problem 4: Custom Training Loops\n",
    "\n",
    "**Problem Statement**: Implement custom training loops with advanced techniques.\n",
    "\n",
    "**Requirements**:\n",
    "- GradientTape for custom gradients\n",
    "- Custom training step with multiple losses\n",
    "- Gradient clipping and accumulation\n",
    "- Advanced regularization during training\n",
    "- Distributed training setup\n",
    "\n",
    "**Key Concepts**: GradientTape, custom training loops, gradient manipulation, distributed strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Custom Training Loop Implementation\n",
    "    class CustomTrainer:\n",
    "        \"\"\"Custom training loop with advanced features.\"\"\"\n",
    "        \n",
    "        def __init__(self, model, optimizer, loss_fn, metrics=None):\n",
    "            self.model = model\n",
    "            self.optimizer = optimizer\n",
    "            self.loss_fn = loss_fn\n",
    "            self.metrics = metrics or []\n",
    "            \n",
    "            # Training state\n",
    "            self.train_loss = keras.metrics.Mean(name='train_loss')\n",
    "            self.train_metrics = [keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')]\n",
    "            \n",
    "            self.val_loss = keras.metrics.Mean(name='val_loss')\n",
    "            self.val_metrics = [keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')]\n",
    "            \n",
    "            # History tracking\n",
    "            self.history = {\n",
    "                'loss': [],\n",
    "                'accuracy': [],\n",
    "                'val_loss': [],\n",
    "                'val_accuracy': []\n",
    "            }\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step(self, x, y, gradient_accumulation_steps=1, clip_norm=None):\n",
    "            \"\"\"Custom training step with gradient accumulation.\"\"\"\n",
    "            \n",
    "            accumulated_gradients = []\n",
    "            \n",
    "            for i in range(gradient_accumulation_steps):\n",
    "                # Calculate batch indices for accumulation\n",
    "                batch_size = tf.shape(x)[0] // gradient_accumulation_steps\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                x_batch = x[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Forward pass\n",
    "                    predictions = self.model(x_batch, training=True)\n",
    "                    \n",
    "                    # Primary loss\n",
    "                    primary_loss = self.loss_fn(y_batch, predictions)\n",
    "                    \n",
    "                    # Add regularization losses\n",
    "                    reg_losses = self.model.losses\n",
    "                    total_loss = primary_loss + tf.add_n(reg_losses) if reg_losses else primary_loss\n",
    "                    \n",
    "                    # Scale loss for gradient accumulation\n",
    "                    scaled_loss = total_loss / gradient_accumulation_steps\n",
    "                \n",
    "                # Compute gradients\n",
    "                gradients = tape.gradient(scaled_loss, self.model.trainable_variables)\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                if i == 0:\n",
    "                    accumulated_gradients = gradients\n",
    "                else:\n",
    "                    accumulated_gradients = [\n",
    "                        acc_grad + grad for acc_grad, grad in zip(accumulated_gradients, gradients)\n",
    "                    ]\n",
    "                \n",
    "                # Update metrics for this micro-batch\n",
    "                self.train_loss.update_state(primary_loss)\n",
    "                for metric in self.train_metrics:\n",
    "                    metric.update_state(y_batch, predictions)\n",
    "            \n",
    "            # Gradient clipping if specified\n",
    "            if clip_norm is not None:\n",
    "                accumulated_gradients, _ = tf.clip_by_global_norm(accumulated_gradients, clip_norm)\n",
    "            \n",
    "            # Apply accumulated gradients\n",
    "            self.optimizer.apply_gradients(zip(accumulated_gradients, self.model.trainable_variables))\n",
    "        \n",
    "        @tf.function\n",
    "        def val_step(self, x, y):\n",
    "            \"\"\"Validation step.\"\"\"\n",
    "            predictions = self.model(x, training=False)\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "            \n",
    "            self.val_loss.update_state(loss)\n",
    "            for metric in self.val_metrics:\n",
    "                metric.update_state(y, predictions)\n",
    "        \n",
    "        def train(self, train_dataset, val_dataset=None, epochs=10, \n",
    "                 gradient_accumulation_steps=1, clip_norm=None, verbose=True):\n",
    "            \"\"\"Custom training loop.\"\"\"\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                if verbose:\n",
    "                    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                \n",
    "                # Reset metrics\n",
    "                self.train_loss.reset_state()\n",
    "                for metric in self.train_metrics:\n",
    "                    metric.reset_state()\n",
    "                \n",
    "                # Training loop\n",
    "                for x_batch, y_batch in train_dataset:\n",
    "                    self.train_step(x_batch, y_batch, gradient_accumulation_steps, clip_norm)\n",
    "                \n",
    "                # Validation loop\n",
    "                if val_dataset is not None:\n",
    "                    self.val_loss.reset_state()\n",
    "                    for metric in self.val_metrics:\n",
    "                        metric.reset_state()\n",
    "                    \n",
    "                    for x_batch, y_batch in val_dataset:\n",
    "                        self.val_step(x_batch, y_batch)\n",
    "                \n",
    "                # Update history\n",
    "                self.history['loss'].append(float(self.train_loss.result()))\n",
    "                self.history['accuracy'].append(float(self.train_metrics[0].result()))\n",
    "                \n",
    "                if val_dataset is not None:\n",
    "                    self.history['val_loss'].append(float(self.val_loss.result()))\n",
    "                    self.history['val_accuracy'].append(float(self.val_metrics[0].result()))\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Loss: {self.train_loss.result():.4f} - \"\n",
    "                          f\"Accuracy: {self.train_metrics[0].result():.4f}\", end=\"\")\n",
    "                    \n",
    "                    if val_dataset is not None:\n",
    "                        print(f\" - Val Loss: {self.val_loss.result():.4f} - \"\n",
    "                              f\"Val Accuracy: {self.val_metrics[0].result():.4f}\")\n",
    "                    else:\n",
    "                        print()\n",
    "    \n",
    "    # Adversarial Training\n",
    "    class AdversarialTrainer(CustomTrainer):\n",
    "        \"\"\"Training with adversarial examples.\"\"\"\n",
    "        \n",
    "        def __init__(self, model, optimizer, loss_fn, epsilon=0.01, metrics=None):\n",
    "            super().__init__(model, optimizer, loss_fn, metrics)\n",
    "            self.epsilon = epsilon\n",
    "        \n",
    "        def generate_adversarial_examples(self, x, y):\n",
    "            \"\"\"Generate adversarial examples using FGSM.\"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(x)\n",
    "                predictions = self.model(x, training=False)\n",
    "                loss = self.loss_fn(y, predictions)\n",
    "            \n",
    "            # Get gradient of loss with respect to input\n",
    "            gradients = tape.gradient(loss, x)\n",
    "            \n",
    "            # Create adversarial examples\n",
    "            signed_grad = tf.sign(gradients)\n",
    "            adversarial_x = x + self.epsilon * signed_grad\n",
    "            adversarial_x = tf.clip_by_value(adversarial_x, 0.0, 1.0)\n",
    "            \n",
    "            return adversarial_x\n",
    "        \n",
    "        @tf.function\n",
    "        def adversarial_train_step(self, x, y):\n",
    "            \"\"\"Training step with adversarial examples.\"\"\"\n",
    "            # Generate adversarial examples\n",
    "            x_adv = self.generate_adversarial_examples(x, y)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass on clean examples\n",
    "                clean_predictions = self.model(x, training=True)\n",
    "                clean_loss = self.loss_fn(y, clean_predictions)\n",
    "                \n",
    "                # Forward pass on adversarial examples\n",
    "                adv_predictions = self.model(x_adv, training=True)\n",
    "                adv_loss = self.loss_fn(y, adv_predictions)\n",
    "                \n",
    "                # Combined loss\n",
    "                total_loss = 0.5 * (clean_loss + adv_loss)\n",
    "                \n",
    "                # Add regularization\n",
    "                reg_losses = self.model.losses\n",
    "                if reg_losses:\n",
    "                    total_loss += tf.add_n(reg_losses)\n",
    "            \n",
    "            # Compute and apply gradients\n",
    "            gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "            \n",
    "            # Update metrics\n",
    "            self.train_loss.update_state(total_loss)\n",
    "            for metric in self.train_metrics:\n",
    "                metric.update_state(y, clean_predictions)\n",
    "    \n",
    "    print(\"âš™ï¸ Custom training loops defined successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ TensorFlow not available - showing custom training concepts\")\n",
    "    print(\"ðŸ’¡ Key concepts: GradientTape, custom gradients, adversarial training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Test Custom Training Loops\n",
    "    print(\"ðŸ§ª Testing Custom Training Loops:\")\n",
    "    \n",
    "    # Use small subset for custom training demo\n",
    "    X_train_custom = X_train[:200]\n",
    "    y_train_custom = y_train[:200]\n",
    "    X_val_custom = X_test[:50]\n",
    "    y_val_custom = y_test[:50]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_custom, y_train_custom))\n",
    "    train_dataset = train_dataset.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val_custom, y_val_custom))\n",
    "    val_dataset = val_dataset.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(f\"Custom training - Training samples: {len(X_train_custom)}\")\n",
    "    print(f\"Custom training - Validation samples: {len(X_val_custom)}\")\n",
    "    \n",
    "    # Test standard custom training\n",
    "    print(\"\\n=== Testing Standard Custom Training ===\")\n",
    "    \n",
    "    # Create simple model for testing\n",
    "    custom_model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(32, 32, 3)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Create custom trainer\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    trainer = CustomTrainer(custom_model, optimizer, loss_fn)\n",
    "    \n",
    "    print(f\"Model parameters: {custom_model.count_params():,}\")\n",
    "    \n",
    "    # Train with custom loop\n",
    "    start_time = time.time()\n",
    "    trainer.train(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        epochs=5,\n",
    "        gradient_accumulation_steps=2,  # Simulate larger batch size\n",
    "        clip_norm=1.0,  # Gradient clipping\n",
    "        verbose=True\n",
    "    )\n",
    "    custom_training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Custom training completed in {custom_training_time:.2f}s\")\n",
    "    print(f\"Final training accuracy: {trainer.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {trainer.history['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    # Test adversarial training (if time permits)\n",
    "    try:\n",
    "        print(\"\\n=== Testing Adversarial Training ===\")\n",
    "        \n",
    "        # Create model for adversarial training\n",
    "        adv_model = keras.Sequential([\n",
    "            layers.Flatten(input_shape=(32, 32, 3)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Create adversarial trainer\n",
    "        adv_optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "        adv_trainer = AdversarialTrainer(\n",
    "            adv_model, adv_optimizer, loss_fn, epsilon=0.01\n",
    "        )\n",
    "        \n",
    "        print(f\"Adversarial model parameters: {adv_model.count_params():,}\")\n",
    "        \n",
    "        # Override train_step to use adversarial training\n",
    "        adv_trainer.train_step = adv_trainer.adversarial_train_step\n",
    "        \n",
    "        # Train with adversarial examples\n",
    "        start_time = time.time()\n",
    "        adv_trainer.train(\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            epochs=3,\n",
    "            verbose=True\n",
    "        )\n",
    "        adv_training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Adversarial training completed in {adv_training_time:.2f}s\")\n",
    "        print(f\"Final adversarial training accuracy: {adv_trainer.history['accuracy'][-1]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Adversarial training skipped: {str(e)}\")\n",
    "        print(\"ðŸ’¡ In production, ensure proper tensor handling and memory management\")\n",
    "    \n",
    "    # Compare with standard Keras training\n",
    "    print(\"\\n=== Comparing with Standard Keras Training ===\")\n",
    "    \n",
    "    # Standard model\n",
    "    standard_model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=(32, 32, 3)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    standard_model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train with standard Keras\n",
    "    start_time = time.time()\n",
    "    standard_history = standard_model.fit(\n",
    "        X_train_custom, y_train_custom,\n",
    "        epochs=5,\n",
    "        batch_size=16,\n",
    "        validation_data=(X_val_custom, y_val_custom),\n",
    "        verbose=1\n",
    "    )\n",
    "    standard_training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Standard training completed in {standard_training_time:.2f}s\")\n",
    "    print(f\"Final standard training accuracy: {standard_history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final standard validation accuracy: {standard_history.history['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\nðŸ“Š Training Method Comparison:\")\n",
    "    print(f\"Custom Training - Time: {custom_training_time:.2f}s, Val Acc: {trainer.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Standard Training - Time: {standard_training_time:.2f}s, Val Acc: {standard_history.history['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    custom_results = {\n",
    "        'custom_history': trainer.history,\n",
    "        'standard_history': standard_history.history,\n",
    "        'custom_time': custom_training_time,\n",
    "        'standard_time': standard_training_time\n",
    "    }\n",
    "    \n",
    "    print(\"\\nâœ… Custom training loop testing completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping custom training tests - TensorFlow not available\")\n",
    "    custom_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸƒâ€â™‚ï¸ Practice Problems\n",
    "\n",
    "Let's practice some additional TensorFlow/Keras concepts commonly asked in interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Problem 5: Model Quantization and Optimization\n",
    "    print(\"ðŸ§ª Testing Model Quantization and Optimization:\")\n",
    "    \n",
    "    # Create a simple model for optimization\n",
    "    opt_model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile and train briefly\n",
    "    opt_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create some dummy data\n",
    "    X_dummy = np.random.random((1000, 784))\n",
    "    y_dummy = np.random.randint(0, 10, (1000,))\n",
    "    \n",
    "    opt_model.fit(X_dummy, y_dummy, epochs=2, verbose=0)\n",
    "    \n",
    "    # Model size analysis\n",
    "    def get_model_size(model):\n",
    "        \"\"\"Estimate model size in MB.\"\"\"\n",
    "        param_count = model.count_params()\n",
    "        # Assume float32 (4 bytes per parameter)\n",
    "        size_mb = (param_count * 4) / (1024 * 1024)\n",
    "        return size_mb\n",
    "    \n",
    "    original_size = get_model_size(opt_model)\n",
    "    print(f\"Original model size: {original_size:.2f} MB\")\n",
    "    print(f\"Original model parameters: {opt_model.count_params():,}\")\n",
    "    \n",
    "    # Test model pruning (conceptual - TF Lite quantization)\n",
    "    try:\n",
    "        # Convert to TensorFlow Lite (quantized)\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(opt_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        quantized_model = converter.convert()\n",
    "        \n",
    "        # Estimate quantized size\n",
    "        quantized_size_mb = len(quantized_model) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"Quantized model size: {quantized_size_mb:.2f} MB\")\n",
    "        print(f\"Size reduction: {((original_size - quantized_size_mb) / original_size * 100):.1f}%\")\n",
    "        \n",
    "        print(\"âœ… Model quantization successful\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Quantization not available: {str(e)}\")\n",
    "    \n",
    "    # Problem 6: Distributed Training Setup (conceptual)\n",
    "    print(\"\\nðŸ§ª Distributed Training Concepts:\")\n",
    "    \n",
    "    try:\n",
    "        # Check available devices\n",
    "        physical_devices = tf.config.list_physical_devices()\n",
    "        print(f\"Available devices: {len(physical_devices)}\")\n",
    "        \n",
    "        for device in physical_devices:\n",
    "            print(f\"  {device.device_type}: {device.name}\")\n",
    "        \n",
    "        # Multi-GPU strategy (if available)\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if len(gpus) > 1:\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            print(f\"\\nUsing MirroredStrategy with {strategy.num_replicas_in_sync} replicas\")\n",
    "            \n",
    "            with strategy.scope():\n",
    "                distributed_model = keras.Sequential([\n",
    "                    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "                    layers.Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "                distributed_model.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "            \n",
    "            print(\"âœ… Distributed model created successfully\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Single device - using default strategy\")\n",
    "            print(\"ðŸ’¡ In production: Use tf.distribute.MirroredStrategy for multi-GPU\")\n",
    "            print(\"ðŸ’¡ Use tf.distribute.MultiWorkerMirroredStrategy for multi-machine\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Device enumeration error: {str(e)}\")\n",
    "    \n",
    "    # Problem 7: Model Checkpointing and Callbacks\n",
    "    print(\"\\nðŸ§ª Advanced Callbacks and Checkpointing:\")\n",
    "    \n",
    "    # Custom callback\n",
    "    class CustomCallback(keras.callbacks.Callback):\n",
    "        \"\"\"Custom callback for advanced monitoring.\"\"\"\n",
    "        \n",
    "        def __init__(self, patience=5, min_delta=0.001):\n",
    "            super().__init__()\n",
    "            self.patience = patience\n",
    "            self.min_delta = min_delta\n",
    "            self.wait = 0\n",
    "            self.best_loss = np.inf\n",
    "        \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            current_loss = logs.get('val_loss', logs.get('loss'))\n",
    "            \n",
    "            if current_loss < self.best_loss - self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait = 0\n",
    "                print(f\"\\n  New best loss: {current_loss:.4f}\")\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    print(f\"\\n  Early stopping triggered after {epoch + 1} epochs\")\n",
    "                    self.model.stop_training = True\n",
    "        \n",
    "        def on_train_begin(self, logs=None):\n",
    "            print(\"ðŸš€ Training started with custom monitoring\")\n",
    "        \n",
    "        def on_train_end(self, logs=None):\n",
    "            print(\"ðŸ Training completed\")\n",
    "    \n",
    "    # Test callbacks\n",
    "    callback_model = keras.Sequential([\n",
    "        layers.Dense(16, activation='relu', input_shape=(10,)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    callback_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create dummy binary classification data\n",
    "    X_cb = np.random.random((200, 10))\n",
    "    y_cb = np.random.randint(0, 2, (200,))\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks_list = [\n",
    "        CustomCallback(patience=3),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            save_best_only=True,\n",
    "            monitor='loss',\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train with callbacks\n",
    "    print(\"Training with advanced callbacks:\")\n",
    "    callback_history = callback_model.fit(\n",
    "        X_cb, y_cb,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining completed after {len(callback_history.history['loss'])} epochs\")\n",
    "    print(\"âœ… Advanced callbacks testing completed\")\n",
    "    \n",
    "    print(\"\\nâœ… All practice problems completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Practice problems skipped - TensorFlow not available\")\n",
    "    print(\"ðŸ’¡ Key concepts: Model optimization, quantization, distributed training, callbacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all TensorFlow/Keras results\n",
    "if TF_AVAILABLE and (model_results or cnn_results or tl_results or custom_results):\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Plot 1: Custom model comparison (if available)\n",
    "    if model_results:\n",
    "        plt.subplot(3, 5, 1)\n",
    "        model_names = [r['name'] for r in model_results]\n",
    "        model_accs = [r['test_results']['accuracy'] for r in model_results]\n",
    "        bars = plt.bar(model_names, model_accs, alpha=0.7)\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('Custom Models Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        for bar, acc in zip(bars, model_accs):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                     f'{acc:.3f}', ha='center', va='bottom')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: CNN architectures comparison (if available)\n",
    "    if cnn_results:\n",
    "        plt.subplot(3, 5, 2)\n",
    "        arch_names = [r['architecture'] for r in cnn_results]\n",
    "        arch_accs = [r['test_accuracy'] for r in cnn_results]\n",
    "        bars = plt.bar(arch_names, arch_accs, alpha=0.7, color='green')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('CNN Architectures')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        for bar, acc in zip(bars, arch_accs):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                     f'{acc:.3f}', ha='center', va='bottom')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Transfer learning comparison (if available)\n",
    "    if tl_results:\n",
    "        plt.subplot(3, 5, 3)\n",
    "        tl_names = [r['name'] for r in tl_results]\n",
    "        tl_accs = [r['test_accuracy'] for r in tl_results]\n",
    "        bars = plt.bar(tl_names, tl_accs, alpha=0.7, color='orange')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('Transfer Learning Strategies')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        for bar, acc in zip(bars, tl_accs):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                     f'{acc:.3f}', ha='center', va='bottom')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Training time comparison\n",
    "    plt.subplot(3, 5, 4)\n",
    "    methods = []\n",
    "    times = []\n",
    "    \n",
    "    if model_results:\n",
    "        methods.extend([r['name'][:8] for r in model_results])\n",
    "        times.extend([r['training_time'] for r in model_results])\n",
    "    \n",
    "    if cnn_results:\n",
    "        methods.extend([r['architecture'][:8] for r in cnn_results])\n",
    "        times.extend([r['training_time'] for r in cnn_results])\n",
    "    \n",
    "    if methods:\n",
    "        bars = plt.bar(methods, times, alpha=0.7, color='red')\n",
    "        plt.ylabel('Training Time (s)')\n",
    "        plt.title('Training Time Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        for bar, time_val in zip(bars, times):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                     f'{time_val:.1f}s', ha='center', va='bottom', fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Model parameters comparison\n",
    "    plt.subplot(3, 5, 5)\n",
    "    param_methods = []\n",
    "    param_counts = []\n",
    "    \n",
    "    if cnn_results:\n",
    "        param_methods.extend([r['architecture'] for r in cnn_results])\n",
    "        param_counts.extend([r['parameters'] for r in cnn_results])\n",
    "    \n",
    "    if tl_results:\n",
    "        param_methods.extend([r['name'][:8] for r in tl_results])\n",
    "        param_counts.extend([r['total_params'] for r in tl_results])\n",
    "    \n",
    "    if param_methods:\n",
    "        bars = plt.bar(param_methods, param_counts, alpha=0.7, color='purple')\n",
    "        plt.ylabel('Parameters (log scale)')\n",
    "        plt.title('Model Size Comparison')\n",
    "        plt.yscale('log')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plots 6-10: Training curves for different methods\n",
    "    curve_data = []\n",
    "    \n",
    "    if model_results:\n",
    "        for result in model_results[:2]:  # Show first 2\n",
    "            curve_data.append({\n",
    "                'name': result['name'],\n",
    "                'history': result['history'].history\n",
    "            })\n",
    "    \n",
    "    if cnn_results:\n",
    "        for result in cnn_results[:2]:  # Show first 2\n",
    "            curve_data.append({\n",
    "                'name': result['architecture'],\n",
    "                'history': result['history'].history\n",
    "            })\n",
    "    \n",
    "    if custom_results:\n",
    "        curve_data.append({\n",
    "            'name': 'Custom Loop',\n",
    "            'history': custom_results['custom_history']\n",
    "        })\n",
    "    \n",
    "    for i, data in enumerate(curve_data[:5]):\n",
    "        plt.subplot(3, 5, 6 + i)\n",
    "        history = data['history']\n",
    "        epochs = range(1, len(history['loss']) + 1)\n",
    "        \n",
    "        plt.plot(epochs, history['loss'], 'b-', label='Training Loss', alpha=0.8)\n",
    "        if 'val_loss' in history:\n",
    "            plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{data[\"name\"]} Training')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plots 11-15: Accuracy curves\n",
    "    for i, data in enumerate(curve_data[:5]):\n",
    "        plt.subplot(3, 5, 11 + i)\n",
    "        history = data['history']\n",
    "        epochs = range(1, len(history['accuracy']) + 1)\n",
    "        \n",
    "        plt.plot(epochs, history['accuracy'], 'g-', label='Training Acc', alpha=0.8)\n",
    "        if 'val_accuracy' in history:\n",
    "            plt.plot(epochs, history['val_accuracy'], 'orange', label='Validation Acc', alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'{data[\"name\"]} Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    print(\"\\nðŸ“Š Complete TensorFlow/Keras Analysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if model_results:\n",
    "        print(\"\\nðŸ§± Custom Models:\")\n",
    "        for result in model_results:\n",
    "            print(f\"  {result['name']:<20}: {result['test_results']['accuracy']:.4f} accuracy, \"\n",
    "                  f\"{result['training_time']:.1f}s training\")\n",
    "    \n",
    "    if cnn_results:\n",
    "        print(\"\\nðŸ—ï¸ CNN Architectures:\")\n",
    "        for result in cnn_results:\n",
    "            print(f\"  {result['architecture'].upper():<20}: {result['test_accuracy']:.4f} accuracy, \"\n",
    "                  f\"{result['parameters']:,} parameters\")\n",
    "    \n",
    "    if tl_results:\n",
    "        print(\"\\nðŸ”„ Transfer Learning:\")\n",
    "        for result in tl_results:\n",
    "            trainable_pct = (result['trainable_params'] / result['total_params']) * 100\n",
    "            print(f\"  {result['name']:<20}: {result['test_accuracy']:.4f} accuracy, \"\n",
    "                  f\"{trainable_pct:.1f}% trainable\")\n",
    "    \n",
    "    if custom_results:\n",
    "        print(\"\\nâš™ï¸ Custom Training:\")\n",
    "        print(f\"  Custom Loop Time: {custom_results['custom_time']:.2f}s\")\n",
    "        print(f\"  Standard Time: {custom_results['standard_time']:.2f}s\")\n",
    "        print(f\"  Custom Final Val Acc: {custom_results['custom_history']['val_accuracy'][-1]:.4f}\")\n",
    "        print(f\"  Standard Final Val Acc: {custom_results['standard_history']['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ“Š Visualization Summary:\")\n",
    "    print(\"âš ï¸ TensorFlow not available or no results to visualize\")\n",
    "    print(\"ðŸ’¡ In interviews, discuss the concepts and show understanding of:\")\n",
    "    print(\"  - Custom layer implementation\")\n",
    "    print(\"  - Advanced CNN architectures (ResNet, SE blocks)\")\n",
    "    print(\"  - Transfer learning strategies\")\n",
    "    print(\"  - Custom training loops with GradientTape\")\n",
    "    print(\"  - Model optimization and deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Interview Tips\n",
    "\n",
    "### ðŸ”¥ TensorFlow/Keras Expertise\n",
    "1. **Understand the ecosystem** - TF, Keras, TF Lite, TF Serving, TF.js\n",
    "2. **Know when to use each approach** - Sequential, Functional, Subclassing APIs\n",
    "3. **Master custom implementations** - Layers, losses, metrics, training loops\n",
    "4. **Optimization techniques** - Mixed precision, quantization, pruning\n",
    "5. **Production deployment** - SavedModel format, TF Serving, edge deployment\n",
    "\n",
    "### âš¡ Common Interview Questions\n",
    "\n",
    "**1. \"Implement a custom layer in Keras\"**\n",
    "- Subclass `tf.keras.layers.Layer`\n",
    "- Implement `build()` and `call()` methods\n",
    "- Handle `training` parameter correctly\n",
    "- Add proper serialization with `get_config()`\n",
    "\n",
    "**2. \"Explain transfer learning strategies\"**\n",
    "- Feature extraction: Freeze base model, train new head\n",
    "- Fine-tuning: Unfreeze some/all layers, use lower learning rate\n",
    "- Progressive unfreezing: Gradually unfreeze layers\n",
    "\n",
    "**3. \"How would you implement custom training logic?\"**\n",
    "- Use `tf.GradientTape()` for custom gradients\n",
    "- Handle forward pass, loss calculation, backpropagation manually\n",
    "- Implement gradient clipping, accumulation as needed\n",
    "\n",
    "**4. \"Optimize a model for mobile deployment\"**\n",
    "- Use TensorFlow Lite converter\n",
    "- Apply quantization (int8, float16)\n",
    "- Consider model architecture (MobileNet, EfficientNet)\n",
    "- Profile memory and compute requirements\n",
    "\n",
    "### ðŸ—ï¸ Architecture Patterns\n",
    "- **Residual connections**: Skip connections for deep networks\n",
    "- **Attention mechanisms**: Multi-head attention, self-attention\n",
    "- **Efficient architectures**: Depthwise separable convolutions\n",
    "- **Regularization**: Dropout, batch normalization, data augmentation\n",
    "\n",
    "### ðŸ“Š Performance Optimization\n",
    "- **Data pipeline**: `tf.data` API, prefetching, parallel processing\n",
    "- **Mixed precision**: Use `tf.keras.mixed_precision` for faster training\n",
    "- **Distributed training**: MirroredStrategy, MultiWorkerMirroredStrategy\n",
    "- **Model optimization**: TF Lite, TensorRT, graph optimization\n",
    "\n",
    "### ðŸ”§ Debugging and Monitoring\n",
    "- **TensorBoard**: Visualize training, profiling, debugging\n",
    "- **tf.debugging**: Assert operations, check numerics\n",
    "- **Callbacks**: Custom monitoring, early stopping, LR scheduling\n",
    "- **Model analysis**: Layer-wise analysis, gradient flow\n",
    "\n",
    "### ðŸš€ Production Considerations\n",
    "- **Model versioning**: Track model lineage and experiments\n",
    "- **Serving**: TF Serving for REST/gRPC APIs\n",
    "- **Monitoring**: Model drift, performance degradation\n",
    "- **A/B testing**: Compare model versions in production\n",
    "\n",
    "### ðŸ§  Advanced Topics\n",
    "- **Custom training loops**: Beyond `model.fit()`\n",
    "- **Graph optimization**: `tf.function`, XLA compilation\n",
    "- **Checkpointing**: Save/restore training state\n",
    "- **Multi-task learning**: Shared representations, multiple outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "âœ… **Custom Layers and Models** - Layer subclassing, custom losses, advanced metrics  \n",
    "âœ… **Advanced CNN Architectures** - ResNet, SE blocks, depthwise convolutions  \n",
    "âœ… **Transfer Learning** - Feature extraction, fine-tuning, progressive unfreezing  \n",
    "âœ… **Custom Training Loops** - GradientTape, adversarial training, gradient manipulation  \n",
    "âœ… **Model Optimization** - Quantization, distributed training, callbacks  \n",
    "âœ… **Production Techniques** - Model serialization, TF Lite conversion, monitoring  \n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "1. Practice implementing custom components from scratch\n",
    "2. Experiment with different architectures and datasets\n",
    "3. Move on to LeetCode-style algorithm problems\n",
    "4. Study deployment and MLOps practices\n",
    "\n",
    "### ðŸ“š Additional Practice\n",
    "- Implement Transformer architectures\n",
    "- Create multi-modal models (vision + text)\n",
    "- Build recommendation systems\n",
    "- Implement GAN architectures\n",
    "\n",
    "### ðŸ”‘ Key Interview Points\n",
    "- **Framework Mastery**: Deep understanding of TensorFlow/Keras APIs\n",
    "- **Custom Implementation**: Can build components from scratch when needed\n",
    "- **Optimization Skills**: Know how to make models faster and smaller\n",
    "- **Production Ready**: Understand deployment and serving considerations\n",
    "- **Debugging Ability**: Can troubleshoot training and performance issues\n",
    "\n",
    "### ðŸ—ï¸ Architecture Expertise\n",
    "- **Modern CNN**: ResNet, DenseNet, EfficientNet principles\n",
    "- **Attention Mechanisms**: Multi-head attention, Transformer blocks\n",
    "- **Transfer Learning**: When and how to adapt pre-trained models\n",
    "- **Optimization**: Mixed precision, quantization, pruning techniques\n",
    "\n",
    "### ðŸ“ˆ Performance & Scalability\n",
    "- **Training Efficiency**: Custom loops, gradient accumulation, distributed training\n",
    "- **Model Optimization**: TF Lite, quantization, model compression\n",
    "- **Serving**: TF Serving, REST APIs, batch inference\n",
    "- **Monitoring**: Model performance, drift detection, A/B testing\n",
    "\n",
    "**Ready for algorithm and system design challenges! ðŸ§©**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}