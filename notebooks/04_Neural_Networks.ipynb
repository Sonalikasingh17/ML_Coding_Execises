{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Neural Networks from Scratch\n",
    "\n",
    "This notebook covers implementing neural networks from scratch, understanding backpropagation, and optimization techniques commonly asked in deep learning interviews.\n",
    "\n",
    "## ðŸ“‹ Table of Contents\n",
    "1. [Neural Network Fundamentals](#neural-network-fundamentals)\n",
    "2. [Activation Functions](#activation-functions)\n",
    "3. [Backpropagation Algorithm](#backpropagation)\n",
    "4. [Optimizers and Regularization](#optimizers-regularization)\n",
    "5. [Convolutional Neural Networks](#cnn-basics)\n",
    "6. [Practice Problems](#practice-problems)\n",
    "7. [Interview Tips](#interview-tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“Š NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ§  Ready for neural network implementations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Problem 1: Neural Network Fundamentals\n",
    "\n",
    "**Problem Statement**: Implement a feedforward neural network with customizable architecture.\n",
    "\n",
    "**Requirements**:\n",
    "- Support for multiple hidden layers\n",
    "- Different activation functions (ReLU, Sigmoid, Tanh)\n",
    "- Forward propagation implementation\n",
    "- Weight initialization strategies\n",
    "- Batch processing capability\n",
    "\n",
    "**Mathematical Foundation**: \n",
    "- Forward pass: $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$, $a^{[l]} = g^{[l]}(z^{[l]})$\n",
    "- Where $g^{[l]}$ is the activation function for layer $l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"Derivative of ReLU function.\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Derivative of tanh function.\"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"Leaky ReLU activation function.\"\"\"\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        \"\"\"Derivative of Leaky ReLU function.\"\"\"\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        # Numerical stability\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "class NeuralNetworkFromScratch:\n",
    "    \"\"\"Feedforward Neural Network implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, activations, learning_rate=0.01, \n",
    "                 weight_init='xavier', random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        layers: list of integers representing number of neurons in each layer\n",
    "        activations: list of activation function names for each layer\n",
    "        learning_rate: learning rate for gradient descent\n",
    "        weight_init: weight initialization strategy ('xavier', 'he', 'random')\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_init = weight_init\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        \n",
    "        for i in range(1, len(layers)):\n",
    "            if weight_init == 'xavier':\n",
    "                # Xavier/Glorot initialization\n",
    "                limit = np.sqrt(6.0 / (layers[i-1] + layers[i]))\n",
    "                self.weights[i] = np.random.uniform(-limit, limit, (layers[i], layers[i-1]))\n",
    "            elif weight_init == 'he':\n",
    "                # He initialization (good for ReLU)\n",
    "                self.weights[i] = np.random.randn(layers[i], layers[i-1]) * np.sqrt(2.0 / layers[i-1])\n",
    "            else:\n",
    "                # Random initialization\n",
    "                self.weights[i] = np.random.randn(layers[i], layers[i-1]) * 0.1\n",
    "            \n",
    "            self.biases[i] = np.zeros((layers[i], 1))\n",
    "        \n",
    "        # Activation function mapping\n",
    "        self.activation_funcs = {\n",
    "            'sigmoid': ActivationFunctions.sigmoid,\n",
    "            'relu': ActivationFunctions.relu,\n",
    "            'tanh': ActivationFunctions.tanh,\n",
    "            'leaky_relu': ActivationFunctions.leaky_relu,\n",
    "            'softmax': ActivationFunctions.softmax\n",
    "        }\n",
    "        \n",
    "        self.activation_derivatives = {\n",
    "            'sigmoid': ActivationFunctions.sigmoid_derivative,\n",
    "            'relu': ActivationFunctions.relu_derivative,\n",
    "            'tanh': ActivationFunctions.tanh_derivative,\n",
    "            'leaky_relu': ActivationFunctions.leaky_relu_derivative\n",
    "        }\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation through the network.\"\"\"\n",
    "        # Store activations for backpropagation\n",
    "        self.activations_cache = {}\n",
    "        self.z_cache = {}\n",
    "        \n",
    "        # Input layer\n",
    "        self.activations_cache[0] = X.T  # Shape: (features, samples)\n",
    "        \n",
    "        # Forward through hidden and output layers\n",
    "        for i in range(1, len(self.layers)):\n",
    "            # Linear transformation\n",
    "            z = self.weights[i] @ self.activations_cache[i-1] + self.biases[i]\n",
    "            self.z_cache[i] = z\n",
    "            \n",
    "            # Apply activation function\n",
    "            activation_func = self.activation_funcs[self.activations[i-1]]\n",
    "            \n",
    "            if self.activations[i-1] == 'softmax':\n",
    "                # Softmax needs special handling for batch processing\n",
    "                a = activation_func(z.T).T  # Transpose for batch processing\n",
    "            else:\n",
    "                a = activation_func(z)\n",
    "            \n",
    "            self.activations_cache[i] = a\n",
    "        \n",
    "        return self.activations_cache[len(self.layers)-1].T  # Shape: (samples, output_neurons)\n",
    "    \n",
    "    def compute_cost(self, Y_true, Y_pred):\n",
    "        \"\"\"Compute the cost function.\"\"\"\n",
    "        m = Y_true.shape[0]\n",
    "        \n",
    "        if self.activations[-1] == 'softmax':\n",
    "            # Cross-entropy loss for multiclass\n",
    "            # Convert labels to one-hot if needed\n",
    "            if len(Y_true.shape) == 1:\n",
    "                Y_true_onehot = np.eye(self.layers[-1])[Y_true]\n",
    "            else:\n",
    "                Y_true_onehot = Y_true\n",
    "            \n",
    "            # Avoid log(0)\n",
    "            Y_pred_clipped = np.clip(Y_pred, 1e-10, 1 - 1e-10)\n",
    "            cost = -np.mean(np.sum(Y_true_onehot * np.log(Y_pred_clipped), axis=1))\n",
    "        else:\n",
    "            # Binary cross-entropy or MSE\n",
    "            if self.activations[-1] == 'sigmoid':\n",
    "                # Binary cross-entropy\n",
    "                Y_pred_clipped = np.clip(Y_pred, 1e-10, 1 - 1e-10)\n",
    "                cost = -np.mean(Y_true * np.log(Y_pred_clipped) + (1 - Y_true) * np.log(1 - Y_pred_clipped))\n",
    "            else:\n",
    "                # Mean Squared Error\n",
    "                cost = np.mean((Y_true - Y_pred) ** 2) / 2\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def backward_propagation(self, X, Y_true):\n",
    "        \"\"\"Backward propagation to compute gradients.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW = {}\n",
    "        db = {}\n",
    "        \n",
    "        # Output layer error\n",
    "        Y_pred = self.activations_cache[len(self.layers)-1].T\n",
    "        \n",
    "        if self.activations[-1] == 'softmax':\n",
    "            # For softmax + cross-entropy, derivative is simplified\n",
    "            if len(Y_true.shape) == 1:\n",
    "                Y_true_onehot = np.eye(self.layers[-1])[Y_true]\n",
    "            else:\n",
    "                Y_true_onehot = Y_true\n",
    "            \n",
    "            dz = (Y_pred - Y_true_onehot) / m\n",
    "            dz = dz.T  # Shape: (output_neurons, samples)\n",
    "        else:\n",
    "            # General case\n",
    "            if self.activations[-1] == 'sigmoid':\n",
    "                dz = (Y_pred - Y_true.reshape(-1, 1)) / m\n",
    "            else:\n",
    "                # For other activations, compute derivative\n",
    "                activation_deriv = self.activation_derivatives[self.activations[-1]]\n",
    "                dz = (Y_pred - Y_true.reshape(-1, 1)) * activation_deriv(self.z_cache[len(self.layers)-1]) / m\n",
    "            \n",
    "            dz = dz.T  # Shape: (output_neurons, samples)\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        for i in range(len(self.layers)-1, 0, -1):\n",
    "            # Compute gradients for current layer\n",
    "            dW[i] = dz @ self.activations_cache[i-1].T\n",
    "            db[i] = np.sum(dz, axis=1, keepdims=True)\n",
    "            \n",
    "            if i > 1:  # Not the first layer\n",
    "                # Compute error for previous layer\n",
    "                da_prev = self.weights[i].T @ dz\n",
    "                \n",
    "                # Apply activation derivative\n",
    "                activation_deriv = self.activation_derivatives[self.activations[i-2]]\n",
    "                dz = da_prev * activation_deriv(self.z_cache[i-1])\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def update_parameters(self, dW, db):\n",
    "        \"\"\"Update weights and biases using gradient descent.\"\"\"\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.weights[i] -= self.learning_rate * dW[i]\n",
    "            self.biases[i] -= self.learning_rate * db[i]\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, batch_size=None, verbose=True, validation_split=0.1):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        # Split data for validation\n",
    "        if validation_split > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=validation_split, random_state=42\n",
    "            )\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "            X_val, y_val = None, None\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'cost': [],\n",
    "            'accuracy': [],\n",
    "            'val_cost': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        m = X_train.shape[0]\n",
    "        \n",
    "        # Set batch size\n",
    "        if batch_size is None:\n",
    "            batch_size = m  # Full batch\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            num_batches = max(1, m // batch_size)\n",
    "            \n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, m)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward propagation\n",
    "                Y_pred = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # Compute cost\n",
    "                cost = self.compute_cost(y_batch, Y_pred)\n",
    "                epoch_cost += cost\n",
    "                \n",
    "                # Backward propagation\n",
    "                dW, db = self.backward_propagation(X_batch, y_batch)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(dW, db)\n",
    "            \n",
    "            # Average cost over batches\n",
    "            epoch_cost /= num_batches\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            train_pred = self.predict(X_train)\n",
    "            train_accuracy = accuracy_score(y_train, train_pred)\n",
    "            \n",
    "            # Store history\n",
    "            self.history['cost'].append(epoch_cost)\n",
    "            self.history['accuracy'].append(train_accuracy)\n",
    "            \n",
    "            # Validation metrics\n",
    "            if X_val is not None:\n",
    "                val_pred_proba = self.forward_propagation(X_val)\n",
    "                val_cost = self.compute_cost(y_val, val_pred_proba)\n",
    "                val_pred = self.predict(X_val)\n",
    "                val_accuracy = accuracy_score(y_val, val_pred)\n",
    "                \n",
    "                self.history['val_cost'].append(val_cost)\n",
    "                self.history['val_accuracy'].append(val_accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Cost: {epoch_cost:.4f} - \"\n",
    "                          f\"Accuracy: {train_accuracy:.4f} - Val_Cost: {val_cost:.4f} - \"\n",
    "                          f\"Val_Accuracy: {val_accuracy:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Cost: {epoch_cost:.4f} - \"\n",
    "                          f\"Accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        return self.forward_propagation(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        probabilities = self.forward_propagation(X)\n",
    "        \n",
    "        if self.activations[-1] == 'softmax':\n",
    "            # Multiclass: return class with highest probability\n",
    "            return np.argmax(probabilities, axis=1)\n",
    "        else:\n",
    "            # Binary: threshold at 0.5\n",
    "            return (probabilities > 0.5).astype(int).flatten()\n",
    "\n",
    "# Test Neural Network implementation\n",
    "print(\"ðŸ§ª Testing Neural Network Implementation:\")\n",
    "\n",
    "# Generate sample data\n",
    "X_class, y_class = make_classification(n_samples=1000, n_features=10, n_classes=3,\n",
    "                                      n_informative=8, n_redundant=2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_class)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create neural network\n",
    "# Architecture: Input(10) -> Hidden(20) -> Hidden(15) -> Output(3)\n",
    "nn = NeuralNetworkFromScratch(\n",
    "    layers=[10, 20, 15, 3],\n",
    "    activations=['relu', 'relu', 'softmax'],\n",
    "    learning_rate=0.01,\n",
    "    weight_init='xavier',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Network architecture: {nn.layers}\")\n",
    "print(f\"Activation functions: {nn.activations}\")\n",
    "print(f\"Total parameters: {sum(w.size for w in nn.weights.values()) + sum(b.size for b in nn.biases.values())}\")\n",
    "\n",
    "# Train the network\n",
    "print(\"\\nTraining neural network...\")\n",
    "start_time = time.time()\n",
    "nn.fit(X_train, y_train, epochs=500, batch_size=32, verbose=True, validation_split=0.15)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nn.predict(X_test)\n",
    "y_pred_proba = nn.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nâœ… Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Final Training Cost: {nn.history['cost'][-1]:.4f}\")\n",
    "print(f\"Final Validation Cost: {nn.history['val_cost'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Neural Network results\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Training history\n",
    "plt.subplot(3, 4, 1)\n",
    "epochs = range(1, len(nn.history['cost']) + 1)\n",
    "plt.plot(epochs, nn.history['cost'], label='Training Cost', alpha=0.8)\n",
    "plt.plot(epochs, nn.history['val_cost'], label='Validation Cost', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Training and Validation Cost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy history\n",
    "plt.subplot(3, 4, 2)\n",
    "plt.plot(epochs, nn.history['accuracy'], label='Training Accuracy', alpha=0.8)\n",
    "plt.plot(epochs, nn.history['val_accuracy'], label='Validation Accuracy', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Confusion matrix\n",
    "plt.subplot(3, 4, 3)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Plot 4: Weight distribution for first layer\n",
    "plt.subplot(3, 4, 4)\n",
    "first_layer_weights = nn.weights[1].flatten()\n",
    "plt.hist(first_layer_weights, bins=30, alpha=0.7, color='green')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('First Layer Weight Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5-8: Activation function comparisons\n",
    "activation_functions = {\n",
    "    'Sigmoid': ActivationFunctions.sigmoid,\n",
    "    'ReLU': ActivationFunctions.relu,\n",
    "    'Tanh': ActivationFunctions.tanh,\n",
    "    'Leaky ReLU': ActivationFunctions.leaky_relu\n",
    "}\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "for i, (name, func) in enumerate(activation_functions.items()):\n",
    "    plt.subplot(3, 4, 5 + i)\n",
    "    y = func(x)\n",
    "    plt.plot(x, y, linewidth=2, alpha=0.8)\n",
    "    plt.title(f'{name} Activation')\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 9: Prediction confidence\n",
    "plt.subplot(3, 4, 9)\n",
    "max_probs = np.max(y_pred_proba, axis=1)\n",
    "plt.hist(max_probs, bins=20, alpha=0.7, color='orange')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 10: Feature importance (based on first layer weights)\n",
    "plt.subplot(3, 4, 10)\n",
    "feature_importance = np.mean(np.abs(nn.weights[1]), axis=0)\n",
    "features = range(len(feature_importance))\n",
    "bars = plt.bar(features, feature_importance, alpha=0.7, color='purple')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Average |Weight|')\n",
    "plt.title('Feature Importance (First Layer)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 11: Learning rate effect (simulated)\n",
    "plt.subplot(3, 4, 11)\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.5]\n",
    "final_costs = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Quick simulation with fewer epochs\n",
    "    nn_temp = NeuralNetworkFromScratch(\n",
    "        layers=[10, 15, 3],\n",
    "        activations=['relu', 'softmax'],\n",
    "        learning_rate=lr,\n",
    "        random_state=42\n",
    "    )\n",
    "    nn_temp.fit(X_train[:200], y_train[:200], epochs=100, verbose=False, validation_split=0)\n",
    "    final_costs.append(nn_temp.history['cost'][-1])\n",
    "\n",
    "plt.semilogx(learning_rates, final_costs, 'o-', alpha=0.8, linewidth=2, markersize=8)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Final Cost')\n",
    "plt.title('Learning Rate vs Final Cost')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 12: Network architecture visualization\n",
    "plt.subplot(3, 4, 12)\n",
    "# Simple visualization of network structure\n",
    "layer_sizes = nn.layers\n",
    "max_neurons = max(layer_sizes)\n",
    "layer_positions = np.arange(len(layer_sizes))\n",
    "\n",
    "for i, size in enumerate(layer_sizes):\n",
    "    y_positions = np.linspace(-max_neurons/2, max_neurons/2, size)\n",
    "    plt.scatter([i] * size, y_positions, s=100, alpha=0.7, \n",
    "                c=plt.cm.viridis(i / len(layer_sizes)))\n",
    "    \n",
    "    # Add connections (simplified)\n",
    "    if i < len(layer_sizes) - 1:\n",
    "        next_y_positions = np.linspace(-max_neurons/2, max_neurons/2, layer_sizes[i+1])\n",
    "        for y1 in y_positions[:min(3, len(y_positions))]:\n",
    "            for y2 in next_y_positions[:min(3, len(next_y_positions))]:\n",
    "                plt.plot([i, i+1], [y1, y2], 'k-', alpha=0.1)\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Neuron Position')\n",
    "plt.title('Network Architecture')\n",
    "plt.xticks(range(len(layer_sizes)), [f'Layer {i}\\n({size} neurons)' \n",
    "                                     for i, size in enumerate(layer_sizes)])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\nðŸ“Š Neural Network Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Architecture: {' -> '.join(map(str, nn.layers))}\")\n",
    "print(f\"Activations: {' -> '.join(nn.activations)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Final training accuracy: {nn.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {nn.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "print(f\"Total parameters: {sum(w.size for w in nn.weights.values()) + sum(b.size for b in nn.biases.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Problem 2: Advanced Optimizers and Regularization\n",
    "\n",
    "**Problem Statement**: Implement advanced optimization algorithms and regularization techniques.\n",
    "\n",
    "**Requirements**:\n",
    "- Adam, RMSprop, and Momentum optimizers\n",
    "- L1/L2 regularization and Dropout\n",
    "- Batch normalization\n",
    "- Learning rate scheduling\n",
    "- Early stopping\n",
    "\n",
    "**Key Concepts**: Adaptive learning rates, gradient momentum, regularization for generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base class for optimizers.\"\"\"\n",
    "    def update(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer.\"\"\"\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params:\n",
    "            params[key] -= self.learning_rate * grads[key]\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    \"\"\"SGD with Momentum optimizer.\"\"\"\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params:\n",
    "            if key not in self.velocity:\n",
    "                self.velocity[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            self.velocity[key] = self.momentum * self.velocity[key] - self.learning_rate * grads[key]\n",
    "            params[key] += self.velocity[key]\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam optimizer.\"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}  # First moment\n",
    "        self.v = {}  # Second moment\n",
    "        self.t = 0   # Time step\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        \n",
    "        for key in params:\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(params[key])\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_corrected = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            \n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_corrected = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "    \"\"\"RMSprop optimizer.\"\"\"\n",
    "    def __init__(self, learning_rate=0.001, decay_rate=0.9, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.cache = {}\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params:\n",
    "            if key not in self.cache:\n",
    "                self.cache[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            self.cache[key] = self.decay_rate * self.cache[key] + (1 - self.decay_rate) * (grads[key] ** 2)\n",
    "            params[key] -= self.learning_rate * grads[key] / (np.sqrt(self.cache[key]) + self.epsilon)\n",
    "\n",
    "class AdvancedNeuralNetwork(NeuralNetworkFromScratch):\n",
    "    \"\"\"Neural Network with advanced optimizers and regularization.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, activations, optimizer='adam', \n",
    "                 l1_reg=0.0, l2_reg=0.0, dropout_rate=0.0,\n",
    "                 weight_init='xavier', random_state=None):\n",
    "        super().__init__(layers, activations, weight_init=weight_init, random_state=random_state)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        if optimizer == 'sgd':\n",
    "            self.optimizer = SGD(learning_rate=0.01)\n",
    "        elif optimizer == 'momentum':\n",
    "            self.optimizer = Momentum(learning_rate=0.01)\n",
    "        elif optimizer == 'adam':\n",
    "            self.optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            self.optimizer = RMSprop(learning_rate=0.001)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {optimizer}\")\n",
    "        \n",
    "        # Regularization parameters\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_masks = {}\n",
    "        self.training = True\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation with dropout.\"\"\"\n",
    "        # Store activations for backpropagation\n",
    "        self.activations_cache = {}\n",
    "        self.z_cache = {}\n",
    "        self.dropout_masks = {}\n",
    "        \n",
    "        # Input layer\n",
    "        self.activations_cache[0] = X.T  # Shape: (features, samples)\n",
    "        \n",
    "        # Forward through hidden and output layers\n",
    "        for i in range(1, len(self.layers)):\n",
    "            # Linear transformation\n",
    "            z = self.weights[i] @ self.activations_cache[i-1] + self.biases[i]\n",
    "            self.z_cache[i] = z\n",
    "            \n",
    "            # Apply activation function\n",
    "            activation_func = self.activation_funcs[self.activations[i-1]]\n",
    "            \n",
    "            if self.activations[i-1] == 'softmax':\n",
    "                a = activation_func(z.T).T\n",
    "            else:\n",
    "                a = activation_func(z)\n",
    "            \n",
    "            # Apply dropout (except for output layer)\n",
    "            if self.training and self.dropout_rate > 0 and i < len(self.layers) - 1:\n",
    "                mask = np.random.rand(*a.shape) > self.dropout_rate\n",
    "                a = a * mask / (1 - self.dropout_rate)  # Inverted dropout\n",
    "                self.dropout_masks[i] = mask\n",
    "            \n",
    "            self.activations_cache[i] = a\n",
    "        \n",
    "        return self.activations_cache[len(self.layers)-1].T\n",
    "    \n",
    "    def compute_cost(self, Y_true, Y_pred):\n",
    "        \"\"\"Compute cost with regularization.\"\"\"\n",
    "        # Base cost\n",
    "        cost = super().compute_cost(Y_true, Y_pred)\n",
    "        \n",
    "        # Add regularization\n",
    "        l1_cost = 0\n",
    "        l2_cost = 0\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            l1_cost += np.sum(np.abs(self.weights[i]))\n",
    "            l2_cost += np.sum(self.weights[i] ** 2)\n",
    "        \n",
    "        cost += self.l1_reg * l1_cost + self.l2_reg * l2_cost / 2\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def backward_propagation(self, X, Y_true):\n",
    "        \"\"\"Backward propagation with regularization.\"\"\"\n",
    "        # Get base gradients\n",
    "        dW, db = super().backward_propagation(X, Y_true)\n",
    "        \n",
    "        # Add regularization gradients\n",
    "        for i in range(1, len(self.layers)):\n",
    "            # L1 regularization\n",
    "            if self.l1_reg > 0:\n",
    "                dW[i] += self.l1_reg * np.sign(self.weights[i])\n",
    "            \n",
    "            # L2 regularization\n",
    "            if self.l2_reg > 0:\n",
    "                dW[i] += self.l2_reg * self.weights[i]\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def update_parameters(self, dW, db):\n",
    "        \"\"\"Update parameters using the optimizer.\"\"\"\n",
    "        # Combine weights and biases for optimizer\n",
    "        params = {}\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            params[f'W{i}'] = self.weights[i]\n",
    "            params[f'b{i}'] = self.biases[i]\n",
    "            grads[f'W{i}'] = dW[i]\n",
    "            grads[f'b{i}'] = db[i]\n",
    "        \n",
    "        # Update using optimizer\n",
    "        self.optimizer.update(params, grads)\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, batch_size=32, validation_split=0.1, \n",
    "            verbose=True, early_stopping_patience=None):\n",
    "        \"\"\"Train with early stopping.\"\"\"\n",
    "        self.training = True\n",
    "        \n",
    "        # Split data for validation\n",
    "        if validation_split > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=validation_split, random_state=42\n",
    "            )\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "            X_val, y_val = None, None\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'cost': [],\n",
    "            'accuracy': [],\n",
    "            'val_cost': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_val_cost = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        best_biases = None\n",
    "        \n",
    "        m = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            num_batches = max(1, m // batch_size)\n",
    "            \n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(m)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, m)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward propagation\n",
    "                Y_pred = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # Compute cost\n",
    "                cost = self.compute_cost(y_batch, Y_pred)\n",
    "                epoch_cost += cost\n",
    "                \n",
    "                # Backward propagation\n",
    "                dW, db = self.backward_propagation(X_batch, y_batch)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(dW, db)\n",
    "            \n",
    "            # Average cost over batches\n",
    "            epoch_cost /= num_batches\n",
    "            \n",
    "            # Calculate training accuracy (without dropout)\n",
    "            self.training = False\n",
    "            train_pred = self.predict(X_train)\n",
    "            train_accuracy = accuracy_score(y_train, train_pred)\n",
    "            self.training = True\n",
    "            \n",
    "            # Store history\n",
    "            self.history['cost'].append(epoch_cost)\n",
    "            self.history['accuracy'].append(train_accuracy)\n",
    "            \n",
    "            # Validation metrics\n",
    "            if X_val is not None:\n",
    "                self.training = False  # Disable dropout for validation\n",
    "                val_pred_proba = self.forward_propagation(X_val)\n",
    "                val_cost = self.compute_cost(y_val, val_pred_proba)\n",
    "                val_pred = self.predict(X_val)\n",
    "                val_accuracy = accuracy_score(y_val, val_pred)\n",
    "                self.training = True\n",
    "                \n",
    "                self.history['val_cost'].append(val_cost)\n",
    "                self.history['val_accuracy'].append(val_accuracy)\n",
    "                \n",
    "                # Early stopping check\n",
    "                if early_stopping_patience is not None:\n",
    "                    if val_cost < best_val_cost:\n",
    "                        best_val_cost = val_cost\n",
    "                        patience_counter = 0\n",
    "                        # Save best weights\n",
    "                        best_weights = {k: v.copy() for k, v in self.weights.items()}\n",
    "                        best_biases = {k: v.copy() for k, v in self.biases.items()}\n",
    "                    else:\n",
    "                        patience_counter += 1\n",
    "                        \n",
    "                        if patience_counter >= early_stopping_patience:\n",
    "                            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                            # Restore best weights\n",
    "                            if best_weights is not None:\n",
    "                                self.weights = best_weights\n",
    "                                self.biases = best_biases\n",
    "                            break\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Cost: {epoch_cost:.4f} - \"\n",
    "                          f\"Accuracy: {train_accuracy:.4f} - Val_Cost: {val_cost:.4f} - \"\n",
    "                          f\"Val_Accuracy: {val_accuracy:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Cost: {epoch_cost:.4f} - \"\n",
    "                          f\"Accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        self.training = False  # Set to inference mode\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions (inference mode).\"\"\"\n",
    "        was_training = self.training\n",
    "        self.training = False  # Disable dropout\n",
    "        predictions = super().predict(X)\n",
    "        self.training = was_training\n",
    "        return predictions\n",
    "\n",
    "# Test Advanced Neural Network\n",
    "print(\"ðŸ§ª Testing Advanced Neural Network with Different Optimizers:\")\n",
    "\n",
    "# Generate more complex dataset\n",
    "X_complex, y_complex = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train_adv, X_test_adv, y_train_adv, y_test_adv = train_test_split(\n",
    "    X_complex, y_complex, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Test different optimizers\n",
    "optimizers = ['sgd', 'momentum', 'adam', 'rmsprop']\n",
    "results = []\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    print(f\"\\n=== Testing {optimizer.upper()} Optimizer ===\")\n",
    "    \n",
    "    # Create network\n",
    "    nn_adv = AdvancedNeuralNetwork(\n",
    "        layers=[2, 10, 10, 1],\n",
    "        activations=['relu', 'relu', 'sigmoid'],\n",
    "        optimizer=optimizer,\n",
    "        l2_reg=0.001,\n",
    "        dropout_rate=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train network\n",
    "    start_time = time.time()\n",
    "    nn_adv.fit(X_train_adv, y_train_adv, epochs=200, batch_size=32,\n",
    "               validation_split=0.15, verbose=False, early_stopping_patience=20)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_adv = nn_adv.predict(X_test_adv)\n",
    "    accuracy_adv = accuracy_score(y_test_adv, y_pred_adv)\n",
    "    \n",
    "    results.append({\n",
    "        'optimizer': optimizer,\n",
    "        'accuracy': accuracy_adv,\n",
    "        'training_time': training_time,\n",
    "        'final_cost': nn_adv.history['cost'][-1],\n",
    "        'epochs_trained': len(nn_adv.history['cost']),\n",
    "        'history': nn_adv.history\n",
    "    })\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy_adv:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Epochs Trained: {len(nn_adv.history['cost'])}\")\n",
    "\n",
    "print(\"\\nâœ… Advanced neural network tests completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimizer comparison\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Optimizer performance comparison\n",
    "plt.subplot(2, 4, 1)\n",
    "optimizers_names = [r['optimizer'] for r in results]\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "bars = plt.bar(optimizers_names, accuracies, alpha=0.7)\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Optimizer Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training time comparison\n",
    "plt.subplot(2, 4, 2)\n",
    "times = [r['training_time'] for r in results]\n",
    "bars = plt.bar(optimizers_names, times, alpha=0.7, color='orange')\n",
    "plt.ylabel('Training Time (s)')\n",
    "plt.title('Training Time Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training curves for all optimizers\n",
    "plt.subplot(2, 4, 3)\n",
    "for result in results:\n",
    "    epochs = range(1, len(result['history']['cost']) + 1)\n",
    "    plt.plot(epochs, result['history']['cost'], \n",
    "             label=result['optimizer'].upper(), alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Cost')\n",
    "plt.title('Training Cost Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Validation curves for all optimizers\n",
    "plt.subplot(2, 4, 4)\n",
    "for result in results:\n",
    "    epochs = range(1, len(result['history']['val_accuracy']) + 1)\n",
    "    plt.plot(epochs, result['history']['val_accuracy'], \n",
    "             label=result['optimizer'].upper(), alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5-8: Decision boundaries for each optimizer\n",
    "def plot_decision_boundary(X, y, model, title, ax):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict_proba(mesh_points)\n",
    "    if Z.shape[1] > 1:\n",
    "        Z = Z[:, 1]  # Take second class probability for binary classification\n",
    "    else:\n",
    "        Z = Z.flatten()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8, edgecolors='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    return scatter\n",
    "\n",
    "# Create models for decision boundary visualization\n",
    "for i, optimizer in enumerate(optimizers):\n",
    "    ax = plt.subplot(2, 4, 5 + i)\n",
    "    \n",
    "    # Create and train a model for visualization\n",
    "    nn_viz = AdvancedNeuralNetwork(\n",
    "        layers=[2, 8, 1],\n",
    "        activations=['relu', 'sigmoid'],\n",
    "        optimizer=optimizer,\n",
    "        l2_reg=0.001,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    nn_viz.fit(X_train_adv, y_train_adv, epochs=150, verbose=False, validation_split=0)\n",
    "    \n",
    "    scatter = plot_decision_boundary(X_test_adv, y_test_adv, nn_viz, \n",
    "                                   f'{optimizer.upper()}\\nAcc: {results[i][\"accuracy\"]:.3f}', ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed comparison table\n",
    "print(\"\\nðŸ“Š Detailed Optimizer Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Optimizer':<12} {'Accuracy':<10} {'Time (s)':<10} {'Epochs':<8} {'Final Cost':<12}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"{result['optimizer'].upper():<12} {result['accuracy']:<10.4f} \"\n",
    "          f\"{result['training_time']:<10.2f} {result['epochs_trained']:<8} \"\n",
    "          f\"{result['final_cost']:<12.4f}\")\n",
    "\n",
    "print(\"\\nðŸ† Best performing optimizer:\", \n",
    "      max(results, key=lambda x: x['accuracy'])['optimizer'].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Problem 3: Simple Convolutional Neural Network\n",
    "\n",
    "**Problem Statement**: Implement basic CNN components for image classification.\n",
    "\n",
    "**Requirements**:\n",
    "- Convolution and pooling layers\n",
    "- Multiple filters and feature maps\n",
    "- Forward propagation through CNN\n",
    "- Flattening for fully connected layers\n",
    "- Handle different padding strategies\n",
    "\n",
    "**Key Concepts**: Convolution operation, pooling, parameter sharing, translation invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "    \"\"\"Simple convolutional layer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_filters, filter_size, stride=1, padding=0):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize filters with small random weights\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.1\n",
    "        self.biases = np.zeros(num_filters)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass through convolutional layer.\n",
    "        input_data shape: (height, width) for single channel or (channels, height, width)\n",
    "        \"\"\"\n",
    "        if len(input_data.shape) == 2:\n",
    "            # Single channel\n",
    "            input_data = input_data.reshape(1, input_data.shape[0], input_data.shape[1])\n",
    "        \n",
    "        channels, height, width = input_data.shape\n",
    "        \n",
    "        # Add padding\n",
    "        if self.padding > 0:\n",
    "            padded_input = np.pad(input_data, \n",
    "                                ((0, 0), (self.padding, self.padding), (self.padding, self.padding)), \n",
    "                                mode='constant')\n",
    "        else:\n",
    "            padded_input = input_data\n",
    "        \n",
    "        padded_height, padded_width = padded_input.shape[1], padded_input.shape[2]\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        output_height = (padded_height - self.filter_size) // self.stride + 1\n",
    "        output_width = (padded_width - self.filter_size) // self.stride + 1\n",
    "        \n",
    "        # Initialize output\n",
    "        output = np.zeros((self.num_filters, output_height, output_width))\n",
    "        \n",
    "        # Perform convolution\n",
    "        for f in range(self.num_filters):\n",
    "            for i in range(output_height):\n",
    "                for j in range(output_width):\n",
    "                    # Extract region\n",
    "                    h_start = i * self.stride\n",
    "                    h_end = h_start + self.filter_size\n",
    "                    w_start = j * self.stride\n",
    "                    w_end = w_start + self.filter_size\n",
    "                    \n",
    "                    region = padded_input[:, h_start:h_end, w_start:w_end]\n",
    "                    \n",
    "                    # For simplicity, we'll average across input channels\n",
    "                    # In practice, filters would have depth equal to input channels\n",
    "                    region_avg = np.mean(region, axis=0)\n",
    "                    \n",
    "                    # Convolution operation\n",
    "                    output[f, i, j] = np.sum(region_avg * self.filters[f]) + self.biases[f]\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "    \"\"\"Max pooling layer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, pool_size=2, stride=None):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride if stride is not None else pool_size\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Forward pass through max pooling layer.\n",
    "        input_data shape: (channels, height, width)\n",
    "        \"\"\"\n",
    "        channels, height, width = input_data.shape\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        output_height = (height - self.pool_size) // self.stride + 1\n",
    "        output_width = (width - self.pool_size) // self.stride + 1\n",
    "        \n",
    "        # Initialize output\n",
    "        output = np.zeros((channels, output_height, output_width))\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for c in range(channels):\n",
    "            for i in range(output_height):\n",
    "                for j in range(output_width):\n",
    "                    h_start = i * self.stride\n",
    "                    h_end = h_start + self.pool_size\n",
    "                    w_start = j * self.stride\n",
    "                    w_end = w_start + self.pool_size\n",
    "                    \n",
    "                    # Max pooling operation\n",
    "                    output[c, i, j] = np.max(input_data[c, h_start:h_end, w_start:w_end])\n",
    "        \n",
    "        return output\n",
    "\n",
    "class SimpleCNN:\n",
    "    \"\"\"Simple CNN for demonstration purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape  # (height, width) for grayscale\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define layers\n",
    "        self.conv1 = ConvolutionalLayer(num_filters=8, filter_size=3, stride=1, padding=1)\n",
    "        self.pool1 = MaxPoolingLayer(pool_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = ConvolutionalLayer(num_filters=16, filter_size=3, stride=1, padding=1)\n",
    "        self.pool2 = MaxPoolingLayer(pool_size=2, stride=2)\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self._calculate_flattened_size()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = AdvancedNeuralNetwork(\n",
    "            layers=[self.flattened_size, 64, num_classes],\n",
    "            activations=['relu', 'softmax'],\n",
    "            optimizer='adam',\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    def _calculate_flattened_size(self):\n",
    "        \"\"\"Calculate the size after convolution and pooling layers.\"\"\"\n",
    "        # Simulate forward pass to get dimensions\n",
    "        dummy_input = np.zeros(self.input_shape)\n",
    "        \n",
    "        # Conv1 + Pool1\n",
    "        conv1_out = self.conv1.forward(dummy_input)\n",
    "        pool1_out = self.pool1.forward(conv1_out)\n",
    "        \n",
    "        # Conv2 + Pool2\n",
    "        conv2_out = self.conv2.forward(pool1_out)\n",
    "        pool2_out = self.pool2.forward(conv2_out)\n",
    "        \n",
    "        self.flattened_size = pool2_out.size\n",
    "        print(f\"Flattened size after conv layers: {self.flattened_size}\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the entire CNN.\"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Single sample forward pass\n",
    "            sample = X[i]\n",
    "            \n",
    "            # Convolutional layers\n",
    "            conv1_out = self.conv1.forward(sample)\n",
    "            pool1_out = self.pool1.forward(conv1_out)\n",
    "            \n",
    "            conv2_out = self.conv2.forward(pool1_out)\n",
    "            pool2_out = self.pool2.forward(conv2_out)\n",
    "            \n",
    "            # Flatten\n",
    "            flattened = pool2_out.flatten()\n",
    "            outputs.append(flattened)\n",
    "        \n",
    "        # Stack outputs\n",
    "        fc_input = np.array(outputs)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        return self.fc.forward_propagation(fc_input)\n",
    "    \n",
    "    def fit(self, X, y, epochs=50, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Train the CNN (simplified - only FC layers are trained).\"\"\"\n",
    "        print(\"Preprocessing data through convolutional layers...\")\n",
    "        \n",
    "        # Forward pass through conv layers to get features\n",
    "        features = []\n",
    "        for i in range(X.shape[0]):\n",
    "            sample = X[i]\n",
    "            \n",
    "            conv1_out = self.conv1.forward(sample)\n",
    "            pool1_out = self.pool1.forward(conv1_out)\n",
    "            \n",
    "            conv2_out = self.conv2.forward(pool1_out)\n",
    "            pool2_out = self.pool2.forward(conv2_out)\n",
    "            \n",
    "            features.append(pool2_out.flatten())\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        print(f\"Feature extraction completed. Shape: {features.shape}\")\n",
    "        print(\"Training fully connected layers...\")\n",
    "        \n",
    "        # Train only the fully connected part\n",
    "        return self.fc.fit(features, y, epochs=epochs, batch_size=batch_size, \n",
    "                          validation_split=validation_split)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Test Simple CNN\n",
    "print(\"ðŸ§ª Testing Simple CNN Implementation:\")\n",
    "\n",
    "# Load digits dataset (8x8 grayscale images)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data.reshape(-1, 8, 8)  # Reshape to 2D images\n",
    "y_digits = digits.target\n",
    "\n",
    "# Use only first 3 classes for simplicity\n",
    "mask = y_digits < 3\n",
    "X_digits = X_digits[mask]\n",
    "y_digits = y_digits[mask]\n",
    "\n",
    "print(f\"Using {len(X_digits)} samples with {len(np.unique(y_digits))} classes\")\n",
    "print(f\"Image shape: {X_digits.shape[1:]}\")\n",
    "\n",
    "# Split data\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train_cnn = X_train_cnn / 16.0  # Max pixel value in digits dataset is 16\n",
    "X_test_cnn = X_test_cnn / 16.0\n",
    "\n",
    "# Create CNN\n",
    "cnn = SimpleCNN(input_shape=(8, 8), num_classes=3)\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\nTraining CNN...\")\n",
    "start_time = time.time()\n",
    "cnn.fit(X_train_cnn, y_train_cnn, epochs=100, batch_size=16, validation_split=0.15)\n",
    "cnn_training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_pred_cnn = cnn.predict(X_test_cnn)\n",
    "cnn_accuracy = accuracy_score(y_test_cnn, y_pred_cnn)\n",
    "\n",
    "print(f\"\\nâœ… CNN training completed in {cnn_training_time:.2f} seconds\")\n",
    "print(f\"CNN Test Accuracy: {cnn_accuracy:.4f}\")\n",
    "\n",
    "# Compare with regular neural network on flattened images\n",
    "print(\"\\nComparing with regular neural network...\")\n",
    "X_train_flat = X_train_cnn.reshape(X_train_cnn.shape[0], -1)\n",
    "X_test_flat = X_test_cnn.reshape(X_test_cnn.shape[0], -1)\n",
    "\n",
    "nn_regular = AdvancedNeuralNetwork(\n",
    "    layers=[64, 32, 16, 3],\n",
    "    activations=['relu', 'relu', 'softmax'],\n",
    "    optimizer='adam',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn_regular.fit(X_train_flat, y_train_cnn, epochs=100, batch_size=16, \n",
    "               validation_split=0.15, verbose=False)\n",
    "y_pred_regular = nn_regular.predict(X_test_flat)\n",
    "regular_accuracy = accuracy_score(y_test_cnn, y_pred_regular)\n",
    "\n",
    "print(f\"Regular NN Test Accuracy: {regular_accuracy:.4f}\")\n",
    "print(f\"CNN vs Regular NN improvement: {cnn_accuracy - regular_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CNN results\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Sample images from each class\n",
    "plt.subplot(3, 4, 1)\n",
    "for i in range(3):\n",
    "    class_samples = X_digits[y_digits == i]\n",
    "    sample_image = class_samples[0]\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.imshow(sample_image, cmap='gray')\n",
    "    plt.title(f'Class {i} Sample')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot 4: Confusion matrix\n",
    "plt.subplot(3, 4, 4)\n",
    "cm_cnn = confusion_matrix(y_test_cnn, y_pred_cnn)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('CNN Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Plot 5: Feature maps from first convolution layer\n",
    "sample_image = X_test_cnn[0]\n",
    "conv1_features = cnn.conv1.forward(sample_image)\n",
    "\n",
    "for i in range(min(4, conv1_features.shape[0])):\n",
    "    plt.subplot(3, 4, 5 + i)\n",
    "    plt.imshow(conv1_features[i], cmap='viridis')\n",
    "    plt.title(f'Conv1 Filter {i+1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot 9: Training curves comparison\n",
    "plt.subplot(3, 4, 9)\n",
    "cnn_epochs = range(1, len(cnn.fc.history['cost']) + 1)\n",
    "regular_epochs = range(1, len(nn_regular.history['cost']) + 1)\n",
    "\n",
    "plt.plot(cnn_epochs, cnn.fc.history['cost'], label='CNN', alpha=0.8)\n",
    "plt.plot(regular_epochs, nn_regular.history['cost'], label='Regular NN', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Cost')\n",
    "plt.title('Training Cost Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 10: Accuracy comparison\n",
    "plt.subplot(3, 4, 10)\n",
    "plt.plot(cnn_epochs, cnn.fc.history['accuracy'], label='CNN', alpha=0.8)\n",
    "plt.plot(regular_epochs, nn_regular.history['accuracy'], label='Regular NN', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 11: Model comparison bar chart\n",
    "plt.subplot(3, 4, 11)\n",
    "models = ['CNN', 'Regular NN']\n",
    "accuracies_comp = [cnn_accuracy, regular_accuracy]\n",
    "bars = plt.bar(models, accuracies_comp, alpha=0.7, color=['blue', 'orange'])\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "\n",
    "for bar, acc in zip(bars, accuracies_comp):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 12: Filter weights visualization\n",
    "plt.subplot(3, 4, 12)\n",
    "filter_weights = cnn.conv1.filters[0]  # First filter\n",
    "plt.imshow(filter_weights, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
    "plt.title('First Conv Filter Weights')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š CNN vs Regular NN Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Model':<15} {'Accuracy':<12} {'Parameters':<12} {'Time (s)':<10}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate parameters (approximate)\n",
    "cnn_params = (cnn.conv1.filters.size + cnn.conv1.biases.size + \n",
    "              cnn.conv2.filters.size + cnn.conv2.biases.size +\n",
    "              sum(w.size for w in cnn.fc.weights.values()) + \n",
    "              sum(b.size for b in cnn.fc.biases.values()))\n",
    "\n",
    "regular_params = (sum(w.size for w in nn_regular.weights.values()) + \n",
    "                  sum(b.size for b in nn_regular.biases.values()))\n",
    "\n",
    "print(f\"{'CNN':<15} {cnn_accuracy:<12.4f} {cnn_params:<12} {cnn_training_time:<10.2f}\")\n",
    "print(f\"{'Regular NN':<15} {regular_accuracy:<12.4f} {regular_params:<12} {'N/A':<10}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"CNN Improvement: +{cnn_accuracy - regular_accuracy:.4f} accuracy\")\n",
    "print(f\"Parameter Efficiency: CNN uses {cnn_params/regular_params:.2f}x parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸƒâ€â™‚ï¸ Practice Problems\n",
    "\n",
    "Let's practice some additional neural network concepts commonly asked in interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4: Implement Batch Normalization\n",
    "class BatchNormalization:\n",
    "    \"\"\"Batch Normalization layer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, momentum=0.9, epsilon=1e-5):\n",
    "        self.num_features = num_features\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = np.ones(num_features)  # Scale parameter\n",
    "        self.beta = np.zeros(num_features)  # Shift parameter\n",
    "        \n",
    "        # Running statistics (for inference)\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through batch normalization.\"\"\"\n",
    "        if self.training:\n",
    "            # Training mode: use batch statistics\n",
    "            batch_mean = np.mean(x, axis=0)\n",
    "            batch_var = np.var(x, axis=0)\n",
    "            \n",
    "            # Normalize\n",
    "            x_normalized = (x - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "            \n",
    "        else:\n",
    "            # Inference mode: use running statistics\n",
    "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma * x_normalized + self.beta\n",
    "\n",
    "# Problem 5: Implement different weight initialization strategies\n",
    "class WeightInitializer:\n",
    "    \"\"\"Different weight initialization strategies.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier_uniform(fan_in, fan_out, shape):\n",
    "        \"\"\"Xavier/Glorot uniform initialization.\"\"\"\n",
    "        limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier_normal(fan_in, fan_out, shape):\n",
    "        \"\"\"Xavier/Glorot normal initialization.\"\"\"\n",
    "        std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "        return np.random.normal(0, std, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def he_uniform(fan_in, shape):\n",
    "        \"\"\"He uniform initialization (good for ReLU).\"\"\"\n",
    "        limit = np.sqrt(6.0 / fan_in)\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def he_normal(fan_in, shape):\n",
    "        \"\"\"He normal initialization (good for ReLU).\"\"\"\n",
    "        std = np.sqrt(2.0 / fan_in)\n",
    "        return np.random.normal(0, std, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def lecun_uniform(fan_in, shape):\n",
    "        \"\"\"LeCun uniform initialization.\"\"\"\n",
    "        limit = np.sqrt(3.0 / fan_in)\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def lecun_normal(fan_in, shape):\n",
    "        \"\"\"LeCun normal initialization.\"\"\"\n",
    "        std = np.sqrt(1.0 / fan_in)\n",
    "        return np.random.normal(0, std, shape)\n",
    "\n",
    "# Test different initialization strategies\n",
    "print(\"ðŸ§ª Testing Weight Initialization Strategies:\")\n",
    "\n",
    "# Generate a challenging dataset\n",
    "X_init, y_init = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train_init, X_test_init, y_train_init, y_test_init = train_test_split(\n",
    "    X_init, y_init, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Test different initialization methods\n",
    "init_methods = ['xavier', 'he', 'random']\n",
    "init_results = []\n",
    "\n",
    "for init_method in init_methods:\n",
    "    print(f\"\\n=== Testing {init_method} initialization ===\")\n",
    "    \n",
    "    # Create network with different initialization\n",
    "    nn_init = AdvancedNeuralNetwork(\n",
    "        layers=[2, 20, 20, 1],\n",
    "        activations=['relu', 'relu', 'sigmoid'],\n",
    "        optimizer='adam',\n",
    "        weight_init=init_method,\n",
    "        dropout_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train network\n",
    "    start_time = time.time()\n",
    "    nn_init.fit(X_train_init, y_train_init, epochs=200, batch_size=32,\n",
    "                validation_split=0.1, verbose=False, early_stopping_patience=15)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_init = nn_init.predict(X_test_init)\n",
    "    accuracy_init = accuracy_score(y_test_init, y_pred_init)\n",
    "    \n",
    "    init_results.append({\n",
    "        'method': init_method,\n",
    "        'accuracy': accuracy_init,\n",
    "        'training_time': training_time,\n",
    "        'epochs_trained': len(nn_init.history['cost']),\n",
    "        'final_cost': nn_init.history['cost'][-1],\n",
    "        'history': nn_init.history\n",
    "    })\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_init:.4f}\")\n",
    "    print(f\"Training time: {training_time:.2f}s\")\n",
    "    print(f\"Epochs trained: {len(nn_init.history['cost'])}\")\n",
    "\n",
    "# Problem 6: Gradient checking\n",
    "def gradient_check(model, X, y, epsilon=1e-7):\n",
    "    \"\"\"Numerical gradient checking for debugging.\"\"\"\n",
    "    print(\"\\nðŸ” Performing Gradient Check:\")\n",
    "    \n",
    "    # Forward pass to compute gradients\n",
    "    model.forward_propagation(X)\n",
    "    dW, db = model.backward_propagation(X, y)\n",
    "    \n",
    "    # Check gradients for first layer weights (subset)\n",
    "    W = model.weights[1]\n",
    "    dW_analytic = dW[1]\n",
    "    \n",
    "    # Numerical gradient computation (check a few weights)\n",
    "    check_indices = [(0, 0), (0, 1), (1, 0)] if W.shape[0] > 1 and W.shape[1] > 1 else [(0, 0)]\n",
    "    \n",
    "    for i, j in check_indices:\n",
    "        # Forward pass with W[i,j] + epsilon\n",
    "        W[i, j] += epsilon\n",
    "        pred_plus = model.forward_propagation(X)\n",
    "        cost_plus = model.compute_cost(y, pred_plus)\n",
    "        \n",
    "        # Forward pass with W[i,j] - epsilon\n",
    "        W[i, j] -= 2 * epsilon\n",
    "        pred_minus = model.forward_propagation(X)\n",
    "        cost_minus = model.compute_cost(y, pred_minus)\n",
    "        \n",
    "        # Restore original weight\n",
    "        W[i, j] += epsilon\n",
    "        \n",
    "        # Numerical gradient\n",
    "        dW_numerical = (cost_plus - cost_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Compare\n",
    "        dW_analytic_val = dW_analytic[i, j]\n",
    "        difference = abs(dW_numerical - dW_analytic_val)\n",
    "        relative_error = difference / (abs(dW_numerical) + abs(dW_analytic_val) + 1e-8)\n",
    "        \n",
    "        print(f\"Weight[{i},{j}]: Numerical={dW_numerical:.8f}, \"\n",
    "              f\"Analytic={dW_analytic_val:.8f}, \"\n",
    "              f\"Relative Error={relative_error:.2e}\")\n",
    "        \n",
    "        if relative_error < 1e-5:\n",
    "            print(\"  âœ… Gradient check passed\")\n",
    "        elif relative_error < 1e-3:\n",
    "            print(\"  âš ï¸ Gradient check warning (acceptable)\")\n",
    "        else:\n",
    "            print(\"  âŒ Gradient check failed\")\n",
    "\n",
    "# Perform gradient check on a small network\n",
    "print(\"\\nðŸ” Gradient Check on Small Network:\")\n",
    "X_small = X_train_init[:10]  # Small batch for gradient check\n",
    "y_small = y_train_init[:10]\n",
    "\n",
    "# Create small network for gradient checking\n",
    "nn_check = AdvancedNeuralNetwork(\n",
    "    layers=[2, 3, 1],\n",
    "    activations=['relu', 'sigmoid'],\n",
    "    optimizer='sgd',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gradient_check(nn_check, X_small, y_small)\n",
    "\n",
    "print(\"\\nâœ… Advanced neural network concepts tested!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize initialization and advanced concepts results\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Initialization method comparison\n",
    "plt.subplot(2, 4, 1)\n",
    "methods = [r['method'] for r in init_results]\n",
    "accuracies_init = [r['accuracy'] for r in init_results]\n",
    "bars = plt.bar(methods, accuracies_init, alpha=0.7, color=['blue', 'green', 'red'])\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Weight Initialization Comparison')\n",
    "\n",
    "for bar, acc in zip(bars, accuracies_init):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training convergence for different initializations\n",
    "plt.subplot(2, 4, 2)\n",
    "for result in init_results:\n",
    "    epochs = range(1, len(result['history']['cost']) + 1)\n",
    "    plt.plot(epochs, result['history']['cost'], \n",
    "             label=result['method'].title(), alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Cost')\n",
    "plt.title('Convergence by Initialization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Training time comparison\n",
    "plt.subplot(2, 4, 3)\n",
    "times_init = [r['training_time'] for r in init_results]\n",
    "bars = plt.bar(methods, times_init, alpha=0.7, color='orange')\n",
    "plt.ylabel('Training Time (s)')\n",
    "plt.title('Training Time by Initialization')\n",
    "\n",
    "for bar, time_val in zip(bars, times_init):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{time_val:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Batch normalization effect demonstration\n",
    "plt.subplot(2, 4, 4)\n",
    "# Simulate batch normalization effect\n",
    "np.random.seed(42)\n",
    "x_before = np.random.normal(5, 3, 1000)  # Shifted and scaled data\n",
    "bn = BatchNormalization(1)\n",
    "x_after = bn.forward(x_before.reshape(-1, 1)).flatten()\n",
    "\n",
    "plt.hist(x_before, bins=30, alpha=0.7, label='Before BatchNorm', density=True)\n",
    "plt.hist(x_after, bins=30, alpha=0.7, label='After BatchNorm', density=True)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Batch Normalization Effect')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5-8: Weight distributions for different initialization methods\n",
    "init_names = ['Xavier', 'He', 'LeCun', 'Random']\n",
    "fan_in, fan_out = 10, 20\n",
    "shape = (fan_out, fan_in)\n",
    "\n",
    "weight_inits = [\n",
    "    WeightInitializer.xavier_normal(fan_in, fan_out, shape),\n",
    "    WeightInitializer.he_normal(fan_in, shape),\n",
    "    WeightInitializer.lecun_normal(fan_in, shape),\n",
    "    np.random.normal(0, 1, shape)  # Standard normal\n",
    "]\n",
    "\n",
    "for i, (weights, name) in enumerate(zip(weight_inits, init_names)):\n",
    "    plt.subplot(2, 4, 5 + i)\n",
    "    plt.hist(weights.flatten(), bins=30, alpha=0.7, density=True)\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'{name} Initialization')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = np.mean(weights)\n",
    "    std_val = np.std(weights)\n",
    "    plt.text(0.05, 0.95, f'Î¼={mean_val:.3f}\\nÏƒ={std_val:.3f}', \n",
    "             transform=plt.gca().transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of all results\n",
    "print(\"\\nðŸ“Š Complete Neural Network Analysis Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nðŸ§  Basic Neural Network:\")\n",
    "print(f\"  Architecture: {' -> '.join(map(str, nn.layers))}\")\n",
    "print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Training Time: {training_time:.2f}s\")\n",
    "\n",
    "print(\"\\nâš™ï¸ Optimizer Comparison:\")\n",
    "for result in results:\n",
    "    print(f\"  {result['optimizer'].upper():<10}: {result['accuracy']:.4f} accuracy, {result['training_time']:.1f}s\")\n",
    "\n",
    "print(\"\\nðŸ—ï¸ Weight Initialization Comparison:\")\n",
    "for result in init_results:\n",
    "    print(f\"  {result['method'].title():<10}: {result['accuracy']:.4f} accuracy, {result['epochs_trained']} epochs\")\n",
    "\n",
    "print(\"\\nðŸ–¼ï¸ CNN vs Regular NN:\")\n",
    "print(f\"  CNN Accuracy: {cnn_accuracy:.4f}\")\n",
    "print(f\"  Regular NN:   {regular_accuracy:.4f}\")\n",
    "print(f\"  CNN Advantage: +{cnn_accuracy - regular_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ† Best Configurations:\")\n",
    "best_optimizer = max(results, key=lambda x: x['accuracy'])\n",
    "best_init = max(init_results, key=lambda x: x['accuracy'])\n",
    "print(f\"  Best Optimizer: {best_optimizer['optimizer'].upper()} ({best_optimizer['accuracy']:.4f})\")\n",
    "print(f\"  Best Initialization: {best_init['method'].title()} ({best_init['accuracy']:.4f})\")\n",
    "print(f\"  CNN showed {cnn_accuracy - regular_accuracy:.4f} improvement over regular NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Interview Tips\n",
    "\n",
    "### ðŸ§  Neural Network Fundamentals\n",
    "1. **Understand the mathematics** - Know forward/backward propagation equations\n",
    "2. **Explain activation functions** - When and why to use each one\n",
    "3. **Know optimization algorithms** - SGD, Momentum, Adam, RMSprop differences\n",
    "4. **Regularization techniques** - L1/L2, Dropout, Batch Normalization\n",
    "5. **Weight initialization** - Why it matters and different strategies\n",
    "\n",
    "### âš¡ Common Interview Questions\n",
    "1. **\"Implement backpropagation from scratch\"**\n",
    "   - Know chain rule, gradient computation\n",
    "   - Handle different activation functions\n",
    "   - Understand matrix dimensions\n",
    "\n",
    "2. **\"Explain vanishing/exploding gradients\"**\n",
    "   - Causes: deep networks, poor initialization, saturating activations\n",
    "   - Solutions: Better initialization, BatchNorm, ResNets, LSTM/GRU\n",
    "\n",
    "3. **\"Why does Adam work better than SGD?\"**\n",
    "   - Adaptive learning rates per parameter\n",
    "   - Momentum and bias correction\n",
    "   - Better handling of sparse gradients\n",
    "\n",
    "4. **\"How does dropout prevent overfitting?\"**\n",
    "   - Forces network to not rely on specific neurons\n",
    "   - Creates ensemble effect\n",
    "   - Only applied during training\n",
    "\n",
    "### ðŸŽ¯ Activation Functions Guide\n",
    "- **ReLU**: Default choice, fast computation, dead neuron problem\n",
    "- **Leaky ReLU**: Fixes dead neuron problem\n",
    "- **Sigmoid**: Output layer for binary classification, saturates\n",
    "- **Tanh**: Zero-centered, still saturates\n",
    "- **Softmax**: Multi-class classification output\n",
    "\n",
    "### ðŸ”§ Debugging Neural Networks\n",
    "1. **Start simple** - Small network, simple data\n",
    "2. **Check gradients** - Numerical gradient checking\n",
    "3. **Monitor training** - Loss curves, accuracy plots\n",
    "4. **Visualization** - Weight distributions, activations\n",
    "5. **Regularization** - Add gradually if overfitting\n",
    "\n",
    "### ðŸ“Š Performance Optimization\n",
    "- **Batch size**: Larger batches for stable gradients, smaller for regularization\n",
    "- **Learning rate**: Most important hyperparameter\n",
    "- **Architecture**: Deeper vs wider networks\n",
    "- **Normalization**: BatchNorm, LayerNorm for stability\n",
    "- **Early stopping**: Prevent overfitting\n",
    "\n",
    "### ðŸ—ï¸ CNN Concepts\n",
    "- **Convolution**: Local connectivity, parameter sharing\n",
    "- **Pooling**: Translation invariance, dimension reduction\n",
    "- **Receptive field**: How much input affects each output\n",
    "- **Feature maps**: What different filters detect\n",
    "\n",
    "### ðŸŽ“ Advanced Topics to Know\n",
    "- **Batch Normalization**: Internal covariate shift\n",
    "- **Skip connections**: ResNet, gradient flow\n",
    "- **Attention mechanisms**: Transformer architecture\n",
    "- **Regularization**: DropConnect, Spectral normalization\n",
    "- **Optimization**: Learning rate scheduling, warm restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "âœ… **Neural Network Fundamentals** - Forward/backward propagation, activation functions  \n",
    "âœ… **Advanced Optimizers** - SGD, Momentum, Adam, RMSprop implementations  \n",
    "âœ… **Regularization Techniques** - L1/L2, Dropout, early stopping  \n",
    "âœ… **Weight Initialization** - Xavier, He, LeCun strategies  \n",
    "âœ… **Convolutional Networks** - Conv layers, pooling, feature extraction  \n",
    "âœ… **Batch Normalization** - Normalization for stable training  \n",
    "âœ… **Gradient Checking** - Debugging backpropagation implementation  \n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "1. Practice implementing networks from memory\n",
    "2. Try different architectures and datasets\n",
    "3. Move on to TensorFlow/Keras implementations\n",
    "4. Study advanced architectures (ResNet, Transformer)\n",
    "\n",
    "### ðŸ“š Additional Practice\n",
    "- Implement LSTM/GRU for sequence modeling\n",
    "- Create attention mechanisms\n",
    "- Build VAEs and GANs\n",
    "- Implement custom loss functions\n",
    "\n",
    "### ðŸ”‘ Key Interview Points\n",
    "- **Mathematical Understanding**: Know the equations, not just the APIs\n",
    "- **Implementation Skills**: Can code backpropagation from scratch\n",
    "- **Debugging Ability**: Can diagnose and fix training issues\n",
    "- **Architecture Knowledge**: Understand different network types\n",
    "- **Optimization Expertise**: Know when and how to tune hyperparameters\n",
    "\n",
    "### ðŸ§  Core Concepts Mastered\n",
    "- **Forward Propagation**: $a^{[l]} = g^{[l]}(W^{[l]}a^{[l-1]} + b^{[l]})$\n",
    "- **Backward Propagation**: Chain rule application for gradient computation\n",
    "- **Cost Functions**: Cross-entropy, MSE, regularization terms\n",
    "- **Activation Functions**: Non-linearity introduction and properties\n",
    "- **Optimization**: Gradient-based parameter updates\n",
    "\n",
    "**Ready for deep learning frameworks! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}